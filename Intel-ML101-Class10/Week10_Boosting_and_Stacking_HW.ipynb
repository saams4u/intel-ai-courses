{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Boosting and Stacking Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The objective is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n",
    "\n",
    "For each record in the dataset it is provided: \n",
    "\n",
    "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration. \n",
    "- Triaxial angular velocity from the gyroscope. \n",
    "- A 561-feature vector with time and frequency domain variables. \n",
    "- Its activity label. \n",
    "\n",
    "More information about the features is available on the website above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:37.499763Z",
     "start_time": "2017-05-10T01:11:37.494556Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = ['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "* Import the data from the file `Human_Activity_Recognition_Using_Smartphones_Data.csv` and examine the shape and data types. For the data types, there will be too many to list each column separately. Rather, aggregate the types by count.\n",
    "* Determine if the float columns need to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:40.978852Z",
     "start_time": "2017-05-10T01:11:38.803273Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = os.sep.join(data_path + ['Human_Activity_Recognition_Using_Smartphones_Data.csv'])\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "The data has quite a few predictor columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:40.990654Z",
     "start_time": "2017-05-10T01:11:40.980224Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "And they're all float values. The only non-float is the categories column, which is what's being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:43.154139Z",
     "start_time": "2017-05-10T01:11:43.146147Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "The minimum and maximum value for the float columns is -1.0 and 1.0, respectively. However, scaling is never required for tree-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:44.300555Z",
     "start_time": "2017-05-10T01:11:44.102410Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Mask to select float columns\n",
    "float_columns = (data.dtypes == np.float)\n",
    "\n",
    "# Verify that the maximum of all float columns is 1.0\n",
    "print( (data.loc[:,float_columns].max()==1.0).all() )\n",
    "\n",
    "# Verify that the minimum of all float columns is -1.0\n",
    "print( (data.loc[:,float_columns].min()==-1.0).all() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "* Integer encode the activities.\n",
    "* Split the data into train and test data sets. Decide if the data will be stratified or not during the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:45.754512Z",
     "start_time": "2017-05-10T01:11:45.334989Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "data['Activity'] = le.fit_transform(data['Activity'])\n",
    "\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:46.020353Z",
     "start_time": "2017-05-10T01:11:46.015161Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data.Activity.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "**NOTE**: We are about to create training and test sets from `data`. On those datasets, we are going to run grid searches over many choices of parameters. This can take some time. In order to shorten the grid search time, feel free to downsample `data` and create `X_train, X_test, y_train, y_test` from the downsampled dataset.\n",
    "\n",
    "Now split the data into train and test data sets. A stratified split was not used here. If there are issues with any of the error metrics on the test set, it can be a good idea to start model fitting over using a stratified split. Boosting is a pretty powerful model, though, so it may not be necessary in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:47.606561Z",
     "start_time": "2017-05-10T01:11:47.494263Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Alternatively, we could stratify the categories in the split, as was done previously\n",
    "feature_columns = [x for x in data.columns if x != 'Activity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[feature_columns], data['Activity'],\n",
    "                 test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:11:49.286980Z",
     "start_time": "2017-05-10T01:11:49.274354Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "* Fit gradient boosted tree models with all parameters set to their defaults the following tree numbers (`n_estimators = [25, 50, 100, 200, 400]`) and evaluate the accuracy on the test data for each of these models. \n",
    "* Plot the accuracy as a function of estimator number.\n",
    "\n",
    "**Note:** This question may take some time to execute, depending on how many different values are fit for estimators. Setting `max_features=4` in the gradient boosting classifier will increase the convergence rate.\n",
    "\n",
    "Also, this is similar to question 3 from week 9, except that there is no such thing as out-of-bag error for boosted models. And the `warm_flag=True` setting has a bug in the gradient boosted model, so don't use it. Simply create the model inside the `for` loop and set the number of estimators at this time. This will make the fitting take a little longer. Additionally, boosting models tend to take longer to fit than bagged ones because the decision stumps must be fit successively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:12:11.212764Z",
     "start_time": "2017-05-10T01:11:51.188370Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "error_list = list()\n",
    "\n",
    "# Iterate through all of the possibilities for number of estimators\n",
    "tree_list = [15, 50, 100, 200, 400]\n",
    "for n_trees in tree_list:\n",
    "    \n",
    "    # Initialize the gradient boost classifier\n",
    "    GBC = GradientBoostingClassifier(n_estimators=n_trees, \n",
    "                                     subsample=0.5,\n",
    "                                     max_features=4,\n",
    "                                     random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    GBC.fit(X_train.values, y_train.values)\n",
    "    y_pred = GBC.predict(X_test)\n",
    "\n",
    "    # Get the error\n",
    "    error = 1. - accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store it\n",
    "    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n",
    "\n",
    "error_df = pd.concat(error_list, axis=1).T.set_index('n_trees')\n",
    "\n",
    "error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:12:41.798817Z",
     "start_time": "2017-05-10T01:12:41.526579Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:12:59.958013Z",
     "start_time": "2017-05-10T01:12:59.668178Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "# Create the plot\n",
    "ax = error_df.plot(marker='o')\n",
    "\n",
    "# Set parameters\n",
    "ax.set(xlabel='n_trees', ylabel='error')\n",
    "ax.set_xlim(0, max(error_df.index)*1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "* Using a grid search with cross-validation, fit a new gradient boosted classifier with the a list of estimators, similar to question 3. Also consider  varying the learning rates (0.1, 0.01, 0.001, etc.), the subsampling value (1.0 or 0.5), and the number of maximum features (1, 2, etc.).\n",
    "* Examine the parameters of the best fit model.\n",
    "* Calculate relevant error metrics on this model and examine the confusion matrix.\n",
    "\n",
    "**Note:** this question may take some time to execute, depending on how many features are associated with the grid search. It is recommended to start with only a few to ensure everything is working correctly and then add more features. Setting `max_features=4` in the gradient boosting classifier will increase the convergence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:13:50.069924Z",
     "start_time": "2017-05-10T01:13:04.015520Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The parameters to be fit--only n_estimators and learning rate\n",
    "# have been varied here for simplicity\n",
    "param_grid = {'n_estimators': [200, 400],\n",
    "              'learning_rate': [0.1, 0.01]}\n",
    "\n",
    "# The grid search object\n",
    "GV_GBC = GridSearchCV(GradientBoostingClassifier(subsample=0.5,\n",
    "                                                 max_features=4,\n",
    "                                                 random_state=42), \n",
    "                      param_grid=param_grid, \n",
    "                      scoring='accuracy',\n",
    "                      n_jobs=-1)\n",
    "\n",
    "# Do the grid search\n",
    "GV_GBC = GV_GBC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:13:58.145246Z",
     "start_time": "2017-05-10T01:13:58.137933Z"
    }
   },
   "outputs": [],
   "source": [
    "# The best model\n",
    "GV_GBC.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error metrics. Classification report is particularly convenient for multi-class cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:00.156367Z",
     "start_time": "2017-05-10T01:13:59.716319Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = GV_GBC.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix. Note that the gradient boosted model has a little trouble distinguishing between activity class 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:02.200468Z",
     "start_time": "2017-05-10T01:14:01.835016Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sns.set_context('talk')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* Create an AdaBoost model and fit it using grid search, much like question 4. Try a range of estimators between 100 and 200.\n",
    "* Compare the errors from AdaBoost to those from the GradientBoostedClassifier.\n",
    "\n",
    "**NOTE:** Setting `max_features=4` in the decision tree classifier used as the base classifier for AdaBoost will increase the convergence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:04.907337Z",
     "start_time": "2017-05-10T01:14:03.445697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ABC = AdaBoostClassifier(DecisionTreeClassifier(max_features=4))\n",
    "\n",
    "param_grid = {'n_estimators': [100, 150, 200],\n",
    "              'learning_rate': [0.01, 0.001]}\n",
    "\n",
    "GV_ABC = GridSearchCV(ABC,\n",
    "                      param_grid=param_grid, \n",
    "                      scoring='accuracy',\n",
    "                      n_jobs=-1)\n",
    "\n",
    "GV_ABC = GV_ABC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:27.449489Z",
     "start_time": "2017-05-10T01:14:27.441823Z"
    }
   },
   "outputs": [],
   "source": [
    "# The best model\n",
    "GV_ABC.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error metrics. Note that the issues with class 1 and 2 appear to have become more problematic and now include class 0. Also note other issues for classes 3 - 5. AdaBoost is very sensitive to outliers, so that could be the problem here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:28.970078Z",
     "start_time": "2017-05-10T01:14:28.950952Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = GV_ABC.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:30.262072Z",
     "start_time": "2017-05-10T01:14:29.908366Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Fit a logistic regression model with regularization. This can be a replica of a model that worked well in the exercises from week 4.\n",
    "* Using `VotingClassifier`, fit the logistic regression model along with either the GratientBoostedClassifier or the AdaBoost model (or both) from questions 4 and 5.\n",
    "* Determine the error as before and compare the results to the appropriate gradient boosted model(s).\n",
    "* Plot the confusion matrix for the best model created in this set of exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:15:02.371973Z",
     "start_time": "2017-05-10T01:14:37.658154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# L2 regularized logistic regression\n",
    "LR_L2 = LogisticRegressionCV(Cs=5, cv=4, penalty='l2').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the errors and confusion matrix for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:16:54.633111Z",
     "start_time": "2017-05-10T01:16:54.616577Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = LR_L2.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:16:55.955121Z",
     "start_time": "2017-05-10T01:16:55.620672Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the stacked model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:18:06.520265Z",
     "start_time": "2017-05-10T01:16:57.388412Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# The combined model--logistic regression and gradient boosted trees\n",
    "estimators = [('LR_L2', LR_L2), ('GBC', GV_GBC)]\n",
    "\n",
    "# Though it wasn't done here, it is often desirable to train \n",
    "# this model using an additional hold-out data set and/or with cross validation\n",
    "VC = VotingClassifier(estimators, voting='soft')\n",
    "VC = VC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the error on the voting classifier. We do a little better here than with either logistic regression or gradient boosted trees alone. However, the fact that logistic regression does almost as well as gradient boosted trees is an important reminder that it's good practice to try the simplest model first. In some cases, its performance will be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:19:50.917331Z",
     "start_time": "2017-05-10T01:19:50.524569Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = VC.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:19:54.468316Z",
     "start_time": "2017-05-10T01:19:54.089180Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "* Import the iris data and examine the features.\n",
    "* We will be using all of them to predict species, but the species feature will need to be integer encoded.\n",
    "* Download iris.csv from https://www.kaggle.com/uciml/iris/data/  and rename it as Iris_Data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the species feature to an integer. This is a quick way to do it using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "* Use `StratifiedShuffleSplit` to split data into train and test sets that are stratified by species. If possible, preserve the indices of the split for question 11 below.\n",
    "* Check the percent composition of each species level for both the train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All data columns except for species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the percent composition of target class in the train and test iris_data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 9\n",
    "\n",
    "* Fit gradient boosted tree models with all parameters set to their defaults the following tree numbers (`n_estimators = [25, 50, 100, 200, 400]`) and evaluate the accuracy on the test data for each of these models. \n",
    "* Plot the accuracy as a function of estimator number.\n",
    "\n",
    "**Note:** This question may take some time to execute, depending on how many different values are fit for estimators. Setting `max_features=4` in the gradient boosting classifier will increase the convergence rate.\n",
    "\n",
    "Also, this is similar to question 9 from week 9, except that there is no such thing as out-of-bag error for boosted models. And the `warm_flag=True` setting has a bug in the gradient boosted model, so don't use it. Simply create the model inside the `for` loop and set the number of estimators at this time. This will make the fitting take a little longer. Additionally, boosting models tend to take longer to fit than bagged ones because the decision stumps must be fit successively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:12:11.212764Z",
     "start_time": "2017-05-10T01:11:51.188370Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:12:41.798817Z",
     "start_time": "2017-05-10T01:12:41.526579Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:12:59.958013Z",
     "start_time": "2017-05-10T01:12:59.668178Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 10\n",
    "\n",
    "* Using a grid search with cross-validation, fit a new gradient boosted classifier with the a list of estimators, similar to question 9. Also consider  varying the learning rates (0.1, 0.01, 0.001, etc.), the subsampling value (1.0 or 0.5), and the number of maximum features (1, 2, etc.).\n",
    "* Examine the parameters of the best fit model.\n",
    "* Calculate relevant error metrics on this model and examine the confusion matrix.\n",
    "\n",
    "**Note:** this question may take some time to execute, depending on how many features are associated with the grid search. It is recommended to start with only a few to ensure everything is working correctly and then add more features. Setting `max_features=4` in the gradient boosting classifier will increase the convergence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:13:50.069924Z",
     "start_time": "2017-05-10T01:13:04.015520Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:13:58.145246Z",
     "start_time": "2017-05-10T01:13:58.137933Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error metrics. Classification report is particularly convenient for multi-class cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:00.156367Z",
     "start_time": "2017-05-10T01:13:59.716319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix. Note that the gradient boosted model has a little trouble distinguishing between species  class 0,1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:02.200468Z",
     "start_time": "2017-05-10T01:14:01.835016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "* Create an AdaBoost model and fit it using grid search, much like question 10. Try a range of estimators between 100 and 200.\n",
    "* Compare the errors from AdaBoost to those from the GradientBoostedClassifier.\n",
    "\n",
    "**NOTE:** Setting `max_features=4` in the decision tree classifier used as the base classifier for AdaBoost will increase the convergence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:04.907337Z",
     "start_time": "2017-05-10T01:14:03.445697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:27.449489Z",
     "start_time": "2017-05-10T01:14:27.441823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error metrics. Note that the issues with class 0,1 and 2 appear to have become more problematic and now include class 0. AdaBoost is very sensitive to outliers, so that could be the problem here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:28.970078Z",
     "start_time": "2017-05-10T01:14:28.950952Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:14:30.262072Z",
     "start_time": "2017-05-10T01:14:29.908366Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "* Fit a logistic regression model with regularization. This can be a replica of a model that worked well in the exercises from week 4.\n",
    "* Using `VotingClassifier`, fit the logistic regression model along with either the GratientBoostedClassifier or the AdaBoost model (or both) from questions 10 and 11.\n",
    "* Determine the error as before and compare the results to the appropriate gradient boosted model(s).\n",
    "* Plot the confusion matrix for the best model created in this set of exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:15:02.371973Z",
     "start_time": "2017-05-10T01:14:37.658154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# L2 regularized logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the errors and confusion matrix for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:16:54.633111Z",
     "start_time": "2017-05-10T01:16:54.616577Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:16:55.955121Z",
     "start_time": "2017-05-10T01:16:55.620672Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the stacked model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:18:06.520265Z",
     "start_time": "2017-05-10T01:16:57.388412Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the error on the voting classifier. We do a little better here than with either logistic regression or gradient boosted trees alone. However, the fact that logistic regression does almost as well as gradient boosted trees is an important reminder that it's good practice to try the simplest model first. In some cases, its performance will be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-10T01:19:50.917331Z",
     "start_time": "2017-05-10T01:19:50.524569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

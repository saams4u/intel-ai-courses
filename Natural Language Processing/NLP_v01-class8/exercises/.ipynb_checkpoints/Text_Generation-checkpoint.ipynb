{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Text Generation with Markov Chains and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Today we'll focus on a few different methods for generating text. Text generation is a challenging problem that even the largest data science teams are still struggling with, so we'll explore some of the most common and accessible methods to solve the problem, starting at a somewhat basic level. The approaches we'll attempt are:\n",
    "\n",
    "* Markov Chains\n",
    "* Recurrent Neural Networks/Long-Short Term Memory Networks (an extension of RNNs)\n",
    "\n",
    "For the later methods, we'll need Keras. If you haven't already installed Keras, you can use:\n",
    "\n",
    "`conda install -c conda-forge keras tensorflow`\n",
    "\n",
    "or in Windows, by using the Anaconda Navigator to install the keras and tensorflow packages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "* Create a class called markov_chain that takes the path of a text file as input. Add a function that preprocesses the text returning a list of lowercased words with all \" and ' removed.\n",
    "* Now add a function that reads the text and creates a dictionary of the form: `{(word1, word2): [word3], (word2, word3): [word4]...}`. If the sequence `(word1, word2, word4)` then shows up later.. we want to end up with `{(word1, word2): [word3, word4], (word2, word3): [word4]...}`. We want to map out how often every word follows each previous pair of words. If `(word1, word2, word3)` appears a second time then we want `{(word1, word2): [word3, word4, word3], (word2, word3): [word4]...}`.\n",
    "* Add a function for generating new text from a seed, that takes in a key (e.g. `(word1, word2)`) as a starting point, then randomly samples one of the words that follows that key. (e.g. word3). Then have that sampled word appended to the \"generated words\" creating a new key (e.g. `(word2, word3)`). Repeat this process over and over to generate a sentence until reaching some sentence length as specified by the user. Return this sentence as a string.\n",
    "* Modify your code to allow more than 2 words in a key. Make that a parameter the user can set (for instance markov_chain(PATH, ngram = 3) would have a dictionary with a format `{(word1, word2, word3): [word4]}`. Does adding more ngrams make the sentences more intelligible? What is the downside to having large keys in a Markov Generator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class markov_chain(object):\n",
    "    \n",
    "    def __init__(self,text_path,ngram=2):\n",
    "        self.ngram = ngram\n",
    "        self.markov_keys = dict()\n",
    "        self.path = text_path\n",
    "        self.text_as_list = None\n",
    "\n",
    "    def preprocess(self):\n",
    "        with open(self.path,'r') as f:\n",
    "            raw = f.read()\n",
    "        self.text_as_list = # student section here\n",
    "\n",
    "    def markov_group_generator(self,text_as_list):\n",
    "        if len(text_as_list) < self.ngram+1:\n",
    "            raise(\"NOT A LONG ENOUGH TEXT!\")\n",
    "            return\n",
    "\n",
    "        for i in range(self.ngram,len(text_as_list)):\n",
    "            yield tuple(text_as_list[i-self.ngram:i+1])\n",
    "\n",
    "    def create_probability_object(self):\n",
    "        # student section here\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def generate_sentence(self, length=25, starting_word_id=None):\n",
    "        # student section here\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "MC = markov_chain('../data/lovecraft.txt',ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "MC.create_probability_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key,value in MC.markov_keys.items():\n",
    "    print(key,value)\n",
    "    print()\n",
    "    i+=1\n",
    "    if i>50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(MC.generate_sentence(length=100, starting_word_id=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Adding more keys in general does make the sentences better... because we have more and more words that must be in a specific order. Eventually, if you increase the key size large enough, you take out most of the randomization because there will only be a few times that 25 words appear in a certain order. You'll end up just writing the input text back again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "Now let's switch gears to do character level text generation using neural nets. We want to frame this as a classification question: \"given this sequence of characters, which type of character should come next?\" Example: Given \"the quick brown fo\" we want to be able to predict that the next character should be \"x\".\n",
    "\n",
    "* Read in the text from: https://s3.amazonaws.com/text-datasets/nietzsche.txt (stored in week08/data/nietzsche.txt). Create a map that assigns every unique character in the text an ID number. Also create one that maps from ID back to character as well.\n",
    "* Convert the text into lists of characters (with some max length) for input and an associated output. For example: if maxlength = 5 and we had the sentence: \"the quick brown fox\", we'd want to split into `X = [['t','h','e',' ','q'],['h','e',' ','q','u']...]` and `y = ['u','i',...]`. Each of these sequences will act as an record-label pair for training our network.\n",
    "* For each sequence, encode the input-output couple as a set of binary arrays. The inputs and output will both be arrays with length equal to the number of unique characters, but only those markers at the positions of IDs that are in the sequence should be non-zero. As an example, \"the quick brown fox\" has 16 unique characters: \n",
    "    \n",
    "    `['t', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', 'b', 'r', 'o', 'w', 'n', 'f', 'o', 'x']`. \n",
    "    \n",
    "    So an array set for an input sequence `qui` would be (if maxlength = 3):\n",
    "    \n",
    "    `[[0, 0, 0, 0, 1, 0, 0, 0, 0, ...],[0, 0, 0, 0, 0, 1, 0, 0, 0, ...],[0, 0, 0, 0, 0, 0, 1, 0, 0, ...]].`\n",
    "    \n",
    "    And the output for `c` would be:\n",
    "    \n",
    "    `[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0...].`\n",
    "    \n",
    "    You'll need the Character-to-ID map we created earlier to do this. The reason the input becomes 3 inputs is that LSTMs want to know about the **arrangement** of the characters... so it matters that 'q' comes before 'u' which comes before 'i' in this case. \n",
    "* Use a sequence length of 10 and generate both the X and y (sequence and associated label) to be used in the LSTM. The choice of 10 is to speed up training, if you have the time, training with a sequence length of 40 does a better job overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "path = '../data/nietzsche.txt'\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = None # student section here\n",
    "indices_char = None # student section here\n",
    "MAX = 40\n",
    "maxlen=MAX\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "def chop_into_sequences(text, char_indices, indices_char, maxlen=20):\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    # student section here\n",
    "    \n",
    "    print('nb sequences:', len(sentences))\n",
    "\n",
    "    print('Vectorization...')\n",
    "    X = None # student section here\n",
    "    y = None # student section here\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "X,y = chop_into_sequences(text, char_indices, indices_char, maxlen=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "X,y = chop_into_sequences(text, char_indices, indices_char, maxlen=MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "* Create a sequential model with Keras (https://keras.io/models/sequential/)\n",
    "* Add one LSTM layer with 128 nodes, with an input shape that's equal to (number of characters to include, number of unique characters)\n",
    "* Add a Dense layer of the shape (number of unique characters)\n",
    "* Add a final 'softmax' activation layer. Then compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Activation, SimpleRNN, GRU, LSTM, Convolution1D, MaxPooling1D, Merge, Dropout\n",
    "from keras.optimizers import SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "* With the model ready, fit it with the sampled X and y's from above. Run it through a few epochs with a batch size of ~128. (This may take a while, so start with just a few epochs).\n",
    "* After the fit, choose a starting seed from the corpus, and force it into the correct shape for the model to predict from (same shape as X above). Do a prediction of the next character, then remove the first character and add the new character and repeat the process 400 times, generating sentences.\n",
    "* I've provided a few pre-trained sets of weights along with the exercise for the Nietzsche dataset, with max length 10 and 40. If you are having trouble getting the model to train, you can also use the \"load_weights\" method of a sequential model, as long as you've properly followed the instructions above and given it the same number of nodes, max length, and character sets as I have. If your training keeps breaking or is taking too long, you can try to load the weights to practice generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def sample(preds, temperature=None):\n",
    "    # helper function to sample an index from a probability array\n",
    "    \n",
    "    # If the user specifies a temperature, we use this to and some more randomization\n",
    "    if temperature:\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "    # if not, we just return the most likely prediction. This is likely to get trapped in loops.\n",
    "    else:\n",
    "        return np.argmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# train the model, output generated text after each iteration\n",
    "try:\n",
    "    model.load_weights(\"LSTM_128_\"+str(MAX)+\"_Nietzsche.h5\")\n",
    "    print(\"LOADED PREVIOUS WEIGHTS FOR THIS MAX LENGTH.\")\n",
    "except:\n",
    "    print(\"NO PREVIOUS WEIGHTS FOR THIS LENGTH.\")\n",
    "    \n",
    "for iteration in range(1, 4):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "    model.save_weights(\"LSTM_128_\"+str(MAX)+\"_Nietzsche.h5\")\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\" -----')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "        print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "This can be used to generate without a fit, as long as the weight files are loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "# student section here\n",
    "\n",
    "\n",
    "# student section ends here    \n",
    "model.load_weights(\"LSTM_128_\"+str(MAX)+\"_Nietzsche.h5\")\n",
    "\n",
    "generated = ''\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "generated += sentence\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated)\n",
    "\n",
    "for i in range(400):\n",
    "    x = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature=0.2)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    #sys.stdout.write(next_char)\n",
    "    #sys.stdout.flush()\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "This clearly needs more training, as it's getting stuck in a lot of \"the so's\" and whatnot. However, we're already getting a lot of recognizable words. Remember, we never actually told it the word \"dance\" is a word. We told it that the letters 'd', 'a', 'n', 'c', and 'e' do sometimes appear around each other though. So it's learning a bit about the English language, from a spelling/phonetic point of view. With more training, it will likely coalesce some more into a more accurate representation with less repetition and fewer spelling errors. It's not perfect though, and it will never truly understand the sentences it's making. It's a great start for text generation though and is fairly state of the art at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "generated"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (intel_nlp)",
   "language": "python",
   "name": "intel_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

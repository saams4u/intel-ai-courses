{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Exercise\n",
    "In this exercise we will use backpropagation to train a multi-layer perceptron (with a single hidden layer).  We will experiment with different patterns and see how quickly or slowly the weights converge.  We will see the impact and interplay of different parameters such as learning rate, number of iterations, and number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the code below so that it creates a multi-layer perceptron with a single hidden layer (with 4 nodes) and trains it via back-propagation.  Specifically your code should:\n",
    "\n",
    "1. Initialize the weights to random values between -1 and 1\n",
    "1. Perform the feed-forward computation\n",
    "1. Compute the loss function\n",
    "1. Calculate the gradients for all the weights via back-propagation\n",
    "1. Update the weight matrices (using a learning_rate parameter)\n",
    "1. Execute steps 2-5 for a fixed number of iterations\n",
    "1. Plot the accuracies and log loss and observe how they change over time\n",
    "\n",
    "\n",
    "Once your code is running, try it for the different patterns below.\n",
    "\n",
    "- Which patterns was the neural network able to learn quickly and which took longer?\n",
    "- What learning rates and numbers of iterations worked well?\n",
    "- If you have time, try varying the size of the hidden layer and experiment with different activation functions (e.g. ReLu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This code below generates two x values and a y value according to different patterns\n",
    "## It also creates a \"bias\" term (a vector of 1s)\n",
    "## The goal is then to learn the mapping from x to y using a neural network via back-propagation\n",
    "\n",
    "num_obs = 500\n",
    "x_mat_1 = np.random.uniform(-1,1,size = (num_obs,2))\n",
    "x_mat_bias = np.ones((num_obs,1))\n",
    "x_mat_full = np.concatenate( (x_mat_1,x_mat_bias), axis=1)\n",
    "\n",
    "# PICK ONE PATTERN BELOW and comment out the rest.\n",
    "\n",
    "# # Circle pattern\n",
    "# y = (np.sqrt(x_mat_full[:,0]**2 + x_mat_full[:,1]**2)<.75).astype(int)\n",
    "\n",
    "# # Diamond Pattern\n",
    "y = ((np.abs(x_mat_full[:,0]) + np.abs(x_mat_full[:,1]))<1).astype(int)\n",
    "\n",
    "# # Centered square\n",
    "# y = ((np.maximum(np.abs(x_mat_full[:,0]), np.abs(x_mat_full[:,1])))<.5).astype(int)\n",
    "\n",
    "# # Thick Right Angle pattern\n",
    "# y = (((np.maximum((x_mat_full[:,0]), (x_mat_full[:,1])))<.5) & ((np.maximum((x_mat_full[:,0]), (x_mat_full[:,1])))>-.5)).astype(int)\n",
    "\n",
    "# # Thin right angle pattern\n",
    "# y = (((np.maximum((x_mat_full[:,0]), (x_mat_full[:,1])))<.5) & ((np.maximum((x_mat_full[:,0]), (x_mat_full[:,1])))>0)).astype(int)\n",
    "\n",
    "\n",
    "print('shape of x_mat_full is {}'.format(x_mat_full.shape))\n",
    "print('shape of y is {}'.format(y.shape))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(x_mat_full[y==1, 0],x_mat_full[y==1, 1], 'ro', label='class 1', color='darkslateblue')\n",
    "ax.plot(x_mat_full[y==0, 0],x_mat_full[y==0, 1], 'bx', label='class 0', color='chocolate')\n",
    "# ax.grid(True)\n",
    "ax.legend(loc='best')\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred, eps=1e-16):\n",
    "    \"\"\"\n",
    "    Loss function we would like to optimize (minimize)\n",
    "    We are using Logarithmic Loss\n",
    "    http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss\n",
    "    \"\"\"\n",
    "    y_pred = np.maximum(y_pred,eps)\n",
    "    y_pred = np.minimum(y_pred,(1-eps))\n",
    "    return -(np.sum(y_true * np.log(y_pred)) + np.sum((1-y_true)*np.log(1-y_pred)))/len(y_true)\n",
    "\n",
    "\n",
    "def forward_pass(W1, W2):\n",
    "    \"\"\"\n",
    "    Does a forward computation of the neural network\n",
    "    Takes the input `x_mat` (global variable) and produces the output `y_pred`\n",
    "    Also produces the gradient of the log loss function\n",
    "    \"\"\"\n",
    "    global x_mat\n",
    "    global y\n",
    "    global num_\n",
    "    # First, compute the new predictions `y_pred`\n",
    "    z_2 = np.dot(x_mat, W_1)\n",
    "    a_2 = sigmoid(z_2)\n",
    "    z_3 = np.dot(a_2, W_2)\n",
    "    y_pred = sigmoid(z_3).reshape((len(x_mat),))\n",
    "    # Now compute the gradient\n",
    "    J_z_3_grad = -y + y_pred\n",
    "    J_W_2_grad = np.dot(J_z_3_grad, a_2)\n",
    "    a_2_z_2_grad = sigmoid(z_2)*(1-sigmoid(z_2))\n",
    "    J_W_1_grad = (np.dot((J_z_3_grad).reshape(-1,1), W_2.reshape(-1,1).T)*a_2_z_2_grad).T.dot(x_mat).T\n",
    "    gradient = (J_W_1_grad, J_W_2_grad)\n",
    "    \n",
    "    # return\n",
    "    return y_pred, gradient\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(loss_vals, accuracies):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    fig.suptitle('Log Loss and Accuracy over iterations')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(loss_vals)\n",
    "    ax.grid(True)\n",
    "    ax.set(xlabel='iterations', title='Log Loss')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(accuracies)\n",
    "    ax.grid(True)\n",
    "    ax.set(xlabel='iterations', title='Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Complete the pseudocode below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Initialize the network parameters\n",
    "\n",
    "np.random.seed(1241)\n",
    "\n",
    "W_1 = \n",
    "W_2 = \n",
    "num_iter = \n",
    "learning_rate = \n",
    "x_mat = x_mat_full\n",
    "\n",
    "\n",
    "loss_vals, accuracies = [], []\n",
    "for i in range(num_iter):\n",
    "    ### Do a forward computation, and get the gradient\n",
    "    \n",
    "    ## Update the weight matrices\n",
    "    \n",
    "    ### Compute the loss and accuracy\n",
    "\n",
    "    ## Print the loss and accuracy for every 200th iteration\n",
    "    \n",
    "plot_loss_accuracy(loss_vals, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Plot the predicted answers with mistakes in yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

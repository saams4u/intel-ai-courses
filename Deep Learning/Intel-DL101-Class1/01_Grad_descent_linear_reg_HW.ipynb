{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Review and Gradient Descent Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook, we will solve a simple linear regression problem by gradient descent.  \n",
    "We will see the effect of the learning rate on the trajectory in parameter space.\n",
    "We will show how Stochastic Gradient Descent (SGD) differs from the standard version, and the effect of \"shuffling\" your data during SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preliminaries - packages to load\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data from a known distribution\n",
    "Below we will generate data a known distribution.  \n",
    "Specifically, the true model is:\n",
    "\n",
    "$Y = b + \\theta_1 X_1 + \\theta_2 X_2 + \\epsilon$\n",
    "\n",
    "$X_1$ and $X_2$ have a uniform distribution on the interval $[0,10]$, while `const` is a vector of ones (representing the intercept term).\n",
    "\n",
    "We set actual values for $b$ ,$\\theta_1$, and $\\theta_2$\n",
    "\n",
    "Here $b=1.5$, $\\theta_1=2$, and $\\theta_2=5$\n",
    "\n",
    "We then generate a vector of $y$-values according to the model and put the predictors together in a \"feature matrix\" `x_mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)  ## This ensures we get the same data if all of the other parameters remain fixed\n",
    "\n",
    "num_obs = 100\n",
    "x1 = np.random.uniform(0,10,num_obs)\n",
    "x2 = np.random.uniform(0,10,num_obs)\n",
    "const = np.ones(num_obs)\n",
    "eps = np.random.normal(0,.5,num_obs)\n",
    "\n",
    "b = 1.5\n",
    "theta_1 = 2\n",
    "theta_2 = 5\n",
    "\n",
    "y = b*const+ theta_1*x1 + theta_2*x2 + eps\n",
    "\n",
    "x_mat = np.array([const,x1,x2]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the \"Right\" answer directly\n",
    "In the below cells we solve for the optimal set of coefficients.  Note that even though the true model is given by:\n",
    "\n",
    "$b=1.5$, $\\theta_1=2$, and $\\theta_2=5$\n",
    "\n",
    "The maximum likelihood (least-squares) estimate from a finite data set may be slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise:\n",
    "Solve the problem two ways: \n",
    "1. By using the scikit-learn LinearRegression model\n",
    "2. Using matrix algebra directly via the formula $\\theta = (X^T X)^{-1}X^Ty$\n",
    "\n",
    "Note: The scikit-learn solver may give a warning message, this can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solve directly using sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression(fit_intercept=False)\n",
    "lr_model.fit(x_mat, y)\n",
    "\n",
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solve by matrix calculation\n",
    "np.linalg.inv(np.dot(x_mat.T,x_mat)).dot(x_mat.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving by Gradient Descent\n",
    "Another way to solve this problem is to use the method of Gradient Descent.  We will explore this method because (as we will see) Neural Networks are trained by Gradient Descent.  Seeing how gradient descent works on a simple example will build intuition and help us understand some of the nuances around setting the learning rate.  We will also explore Stochastic Gradient Descent and compare its behavior to the standard approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The next several cells have code to perform (full-batch) gradient descent.  We have omitted some parameters for you to fill in.\n",
    "\n",
    "1. Pick a learning rate, and a number of iterations, run the code, and then plot the trajectory of your gradient descent.\n",
    "1. Find examples where the learning rate is too high, too low, and \"just right\".\n",
    "1. Look at plots of loss function under these conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Parameters to play with \n",
    "learning_rate = .00001\n",
    "num_iter = 1000\n",
    "theta_initial = np.array([3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialization steps\n",
    "theta = theta_initial\n",
    "theta_path = np.zeros((num_iter+1,3))\n",
    "theta_path[0,:]= theta_initial\n",
    "\n",
    "loss_vec = np.zeros(num_iter)\n",
    "\n",
    "## Main Gradient Descent loop (for a fixed number of iterations)\n",
    "for i in range(num_iter):\n",
    "    y_pred = np.dot(theta.T,x_mat.T)\n",
    "    loss_vec[i] = np.sum((y-y_pred)**2)\n",
    "    grad_vec = (y-y_pred).dot(x_mat)/num_obs  #sum up the gradients across all observations and divide by num_obs\n",
    "    grad_vec = grad_vec\n",
    "    theta = theta + learning_rate*grad_vec\n",
    "    theta_path[i+1,:]=theta\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "## Plot the results - it is a 3d parameter space - we plot 2d slices\n",
    "## Green is starting point and blue is ending point\n",
    "plt.figure(figsize = (30,20))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(theta_path[:,1],theta_path[:,2],'k-x')\n",
    "plt.plot(theta_path[0,1],theta_path[0,2],'yo')\n",
    "plt.plot(theta_path[-1,1],theta_path[-1,2],'bo')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(theta_path[:,0],theta_path[:,1],'k-x')\n",
    "plt.plot(theta_path[0,0],theta_path[0,1],'yo')\n",
    "plt.plot(theta_path[-1,0],theta_path[-1,1],'bo')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(theta_path[:,0],theta_path[:,2],'k-x')\n",
    "plt.plot(theta_path[0,0],theta_path[0,2],'yo')\n",
    "plt.plot(theta_path[-1,0],theta_path[-1,2],'bo')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(loss_vec)\n",
    "plt.ylim([0,500])\n",
    "\n",
    "## Plot the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "Rather than average the gradients across the whole dataset before taking a step, we will now take a step for every datapoint.  Each step will be somewhat of an \"overreaction\" but they should average out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "The below code runs Stochastic Gradient descent, but runs through the data in the same order every time.  \n",
    "\n",
    "1. Run the code and plot the graphs.  What do you notice?\n",
    "2. Modify the code so that it randomly re-orders the data.  How do the sample trajectories compare? [STUDENT TO COMPLETE THIS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Parameters to play with\n",
    "learning_rate = .002\n",
    "num_iter = 10 #The number of \"steps\" will be num_iter * numobs\n",
    "theta_initial = np.array([3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialization steps\n",
    "theta = theta_initial\n",
    "theta_path = np.zeros(((num_iter*num_obs)+1,3))\n",
    "theta_path[0,:]= theta_initial\n",
    "loss_vec = np.zeros(num_iter*num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Main SGD loop\n",
    "count = 0\n",
    "for i in range(num_iter):\n",
    "    for j in range(num_obs):\n",
    "        count+=1\n",
    "        y_pred = np.dot(theta.T,x_mat.T)\n",
    "        loss_vec[count-1] = np.sum((y-y_pred)**2)\n",
    "        grad_vec = (y[j]-y_pred[j])*(x_mat[j,:])\n",
    "        theta = theta + learning_rate*grad_vec\n",
    "        theta_path[count,:]=theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the results - it is a 3d parameter space - we plot 2d slices\n",
    "## Green is starting point and blue is ending point\n",
    "plt.figure(figsize = (30,20))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(theta_path[:,1],theta_path[:,2],'k-x')\n",
    "plt.plot(theta_path[0,1],theta_path[0,2],'yo')\n",
    "plt.plot(theta_path[-1,1],theta_path[-1,2],'bo')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(theta_path[:,0],theta_path[:,1],'k-x')\n",
    "plt.plot(theta_path[0,0],theta_path[0,1],'yo')\n",
    "plt.plot(theta_path[-1,0],theta_path[-1,1],'bo')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(theta_path[:,0],theta_path[:,2],'k-x')\n",
    "plt.plot(theta_path[0,0],theta_path[0,2],'yo')\n",
    "plt.plot(theta_path[-1,0],theta_path[-1,2],'bo')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(loss_vec)\n",
    "plt.ylim([0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Student to write code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

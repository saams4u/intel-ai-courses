{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3ed72d58-7719-40d6-a229-73778f445d4a"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In the previous four lessons, you learned about several fundamental time series concepts including stationarity, smoothing, trend, seasonality, and autocorrelation, and you built two different kinds of models: \n",
    "\n",
    "  * **MA models**: Specify that the current value of the series depends linearly on the series' mean and a set of prior (observed) white noise error terms.\n",
    "  * **AR models**: Specify that the current value of the  series depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term).\n",
    "\n",
    "In the current lesson we will review thes concepts as well as combine these two model types into three more complicated time series models: ARMA, ARIMA, and SARIMA.\n",
    "\n",
    "# Learning Outcomes\n",
    "You should walk away from this tutorial with:\n",
    "1. A practical understanding of Autoregressive Moving Average (ARMA) models.\n",
    "2. A basic understanding of the Autocorrelation Function (ACF).\n",
    "3. Insight into choosing the order *q* of MA models.\n",
    "4. A practical understanding of Autoregressive (AR) models.\n",
    "5. A basic understanding of the Partial Autocorrelation Function (PACF).\n",
    "6. Insight into choosing the order *p* of AR models.\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyflux as pf\n",
    "import statsmodels as ss\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Library Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "versions = ( (\"matplotlib\", matplotlib.__version__),\n",
    "            (\"numpy\", np.__version__),\n",
    "            (\"pandas\", pd.__version__),\n",
    "            (\"pyflux\", pf.__version__),\n",
    "            (\"seaborn\", sns.__version__),\n",
    "            (\"statsmodels\", ss.__version__) )\n",
    "\n",
    "print(sys.version, \"\\n\")\n",
    "print(\"library\" + \" \" * 4 + \"version\")\n",
    "print(\"-\" * 18)\n",
    "\n",
    "for tup1, tup2 in versions:\n",
    "    print(\"{:11} {}\".format(tup1, tup2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Time Series Review\n",
    "\n",
    "We've covered lots of material in the previous four lessons. Now is a good time to step back and rehash what we've covered. This will help to both solidify concepts and ensure you're ready to tackle ARMA, ARIMA, and SARIMA models.\n",
    "\n",
    "### Section 1.1: Examples of time series data and modeling:\n",
    "- Hedge fund prediction of stock and index movements\n",
    "- Long and short-term weather forecasting\n",
    "- Business budgeting and trend analysis\n",
    "- Health vitals monitoring\n",
    "- Traffic flows and logistic optimization modeling\n",
    "- Can you think of others?\n",
    "\n",
    "### Section 1.2: Decomposition\n",
    "Time series data can often be decomposed into trend, seasonal, and random fluctuation components.\n",
    "\n",
    "![Decomposition](http://rstatistics.net/wp-content/uploads/2014/09/Multiplicative-Decomposition-of-Time-series.png)\n",
    "\n",
    "- Trends\n",
    "    - Up\n",
    "    - Down\n",
    "    - Flat\n",
    "    - Larger trends can be made up of smaller trends\n",
    "    - There is no defined timeframe for what constitutes a trend as it depends on your goals\n",
    "- Seasonal Effects\n",
    "    - Weekend retail sales spikes\n",
    "    - Holiday shopping\n",
    "    - Energy requirement changes with annual weather patterns\n",
    "    - Note: Twitter spikes when news happens are not seasonal because they aren't regular and predictable\n",
    "- Random Fluctuations\n",
    "    - The human element\n",
    "    - Aggregations of small influencers\n",
    "    - Observation errors\n",
    "    - The smaller this is in relation to Trend and Seasonal, the better we can predict the future\n",
    "\n",
    "### Section 1.3: Additive vs Multiplicative\n",
    "Time series models fall into [two camps](http://www.abs.gov.au/websitedbs/D3310114.nsf/home/Time+Series+Analysis:+The+Basics#HOW%20DO%20I%20KNOW%20WHICH%20DECOMPOSITION):\n",
    "- Additive\n",
    "    - Data = Trend + Seasonal + Random\n",
    "    - What we will be using for our modeling\n",
    "- Multiplicative\n",
    "    - Data = Trend x Seasonal x Random\n",
    "    - As easy to fit as Additive if we take the log\n",
    "        - log(Data) = log(Trend x Seasonal x Random)\n",
    "\n",
    "We should use multiplicative models when the percentage change of our data is more important than the absolute value change (e.g. stocks, commodities); as the trend rises and our values grow, we see amplitude growth in seasonal and random fluctuations. If our seasonality and fluctuations are stable, we likely have an additive model.\n",
    "\n",
    "### Section 1.4: Time Series Modeling Process\n",
    "Time series model selection is driven by the Trend and Seasonal components of our raw data. The general approach for analysis looks like this:\n",
    "\n",
    "1. Plot the data and determine Trends and Seasonality\n",
    "    1. Difference or take the log of the data (multiple times if needed) to remove trends for [certain model applications](https://en.wikipedia.org/wiki/Stationary_process)\n",
    "    1. Stationairity is needed for ARMA models\n",
    "1. Determine if we have additive or multiplicative data patterns\n",
    "1. Select the appropriate algorithm based on the chart below\n",
    "1. Determine if model selection is correct with these tools\n",
    "    - Ljung-Box Test\n",
    "    - Residual Errors (Normal Distribution with zero mean and constant variance-homoskedastic, i.i.d)\n",
    "    - Autocorrelation Function (ACF)\n",
    "    - Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Algorithm | Trend | Seasonal | Correlations\n",
    "---|---|---|---\n",
    "ARIMA | X |X|X\n",
    "SMA Smoothing |X||\n",
    "Simple Exponential Smoothing |X||\n",
    "Seasonal Adjustment |X|X|\n",
    "Holt's Exponential Smoothing |X||\n",
    "Holt-Winters |X|X|\n",
    "\n",
    "### Section 1.5: How to Achieve and Test for Stationarity\n",
    "- The mean of the series is not a function of time.\n",
    "\n",
    "- The variance of the series is not a function of time (homoscedasticity).\n",
    "\n",
    "- The covariance at different lags is not a function of time.\n",
    "\n",
    "[From A Complete Tutorial on Time Series Modeling in R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)\n",
    "\n",
    "- [Info on stationarity](http://www.investopedia.com/articles/trading/07/stationary.asp)\n",
    "- Plotting Rolling Statistics\n",
    "    - Plot the moving average/variance and see if it changes with time. This visual technique can be done on different windows, but isn't as rigorously defensible as the test below.\n",
    "- Augmented Dickey-Fuller Test\n",
    "    - Statistical tests for checking stationarity; the null hypothesis is that the TS is non-stationary. If our test statistic is below an `alpha` value, we _can_ reject the null hypothesis and say that the series is stationary.\n",
    "    \n",
    "    $$ Y_t = \\rho * Y_{t-1} + \\epsilon_t $$\n",
    "    \n",
    "    $$ Y_t - Y_{t-1} = (\\rho - 1) Y_{t - 1} + \\epsilon_t $$\n",
    "    \n",
    "### Section 1.6: Differencing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d2e075c0-fccd-4d2c-ba33-1ca65629bd24"
    }
   },
   "outputs": [],
   "source": [
    "# create a play dataframe from 1-10 (linear and squared) to test how differencing works\n",
    "play = pd.DataFrame([[x for x in range(1,11)], [x**2 for x in range(1,11)]]).T\n",
    "play.columns = ['original', 'squared']\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "9f057e12-cf7f-456e-b1cb-9947c891cc4a"
    }
   },
   "outputs": [],
   "source": [
    "# stationarize linear series (mean and variance don't change for sub-windows)\n",
    "play.original.diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This is similar to taking a first-order derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "44e98900-d54d-4e5d-b6b6-5c1f24159247"
    }
   },
   "outputs": [],
   "source": [
    "# stationarize squared series\n",
    "play.squared.diff().diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Notice we need to difference twice on an exponential trend, and every time we do, we lose a bit of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2f7a325d-0c17-4218-aa6d-68e328712d46"
    }
   },
   "outputs": [],
   "source": [
    "# stationarize squared with log\n",
    "np.log(play.squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Works somewhat but certainly not as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d13da4fe-988f-4931-ac85-94b88fdf5a5c"
    }
   },
   "source": [
    "## Data Prep and EDA\n",
    "\n",
    "We'll be looking at [monthly average temperatures between 1907-1972](https://datamarket.com/data/set/22o4/mean-monthly-temperature-1907-1972#!ds=22o4&display=line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ab62c237-c818-4345-82df-40941025361e"
    }
   },
   "outputs": [],
   "source": [
    "# load data and convert to datetime\n",
    "monthly_temp = pd.read_csv('../Data/mean-monthly-temperature-1907-19.csv', \n",
    "                           skipfooter=2, \n",
    "                           infer_datetime_format=True, \n",
    "                           header=0, \n",
    "                           index_col=0, \n",
    "                           names=['month', 'temp'])\n",
    "\n",
    "monthly_temp.index = pd.to_datetime(monthly_temp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "19e5de40-e7b6-42e4-b5b3-90f806d7aed7"
    }
   },
   "outputs": [],
   "source": [
    "monthly_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "0a23aa65-41fa-476c-af07-d844bc712160"
    }
   },
   "outputs": [],
   "source": [
    "# describe\n",
    "monthly_temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resample to annual and plot each\n",
    "plt.rcParams['figure.figsize'] = [14, 4]\n",
    "annual_temp = monthly_temp.resample('A').mean()\n",
    "monthly_temp.plot(grid=True)\n",
    "annual_temp.plot(grid=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot both on same figure\n",
    "plt.plot(monthly_temp)\n",
    "plt.plot(annual_temp)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "e1676068-bb3a-4801-85b0-8ba35c55ea80"
    }
   },
   "outputs": [],
   "source": [
    "# violinplot of months to determine variance and range\n",
    "sns.violinplot(x=monthly_temp.index.month, y=monthly_temp.temp)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5883bb95-161b-4f1c-b56d-6741e1736ffb"
    }
   },
   "source": [
    "Are these datasets stationary? We can look at a few things per the list above, including a visual check (there seems to be a small upward trend in the annual, too hard to tell for monthly), a standard deviation check on various differences (smallest one is usually most stationary), and the formal Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "addeaa9a-90da-4f1c-ab74-ea11bcba542b"
    }
   },
   "outputs": [],
   "source": [
    "# check montly deviations for various diffs\n",
    "print(monthly_temp.temp.std())\n",
    "print(monthly_temp.temp.diff().std())\n",
    "print(monthly_temp.temp.diff().diff().std()) # theoretically lowest, but > 1 is close enough\n",
    "print(monthly_temp.temp.diff().diff().diff().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "14e8e47b-3ced-40b5-971a-4a339941db9d"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# check annual deviations for various diffs\n",
    "print(annual_temp.temp.std()) # looks stationary as is\n",
    "print(annual_temp.temp.diff().std())\n",
    "print(annual_temp.temp.diff().diff().std())\n",
    "print(annual_temp.temp.diff().diff().diff().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "318b7e80-f2a6-4d35-8278-8b8d304011ef"
    }
   },
   "outputs": [],
   "source": [
    "# define Dickey-Fuller Test (DFT) function\n",
    "import statsmodels.tsa.stattools as ts\n",
    "def dftest(timeseries):\n",
    "    dftest = ts.adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], \n",
    "                         index=['Test Statistic','p-value','Lags Used','Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=12).mean()\n",
    "    rolstd = timeseries.rolling(window=12).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.grid()\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d043ef61-dc41-46b3-86af-aaca9d21bde0"
    }
   },
   "outputs": [],
   "source": [
    "# run DFT on monthly\n",
    "dftest(monthly_temp.temp)\n",
    "# p-value allows us to reject a unit root: data is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "0d4cbef8-0e6c-4074-868d-e28e9c51aaa9"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# run DFT on annual\n",
    "dftest(annual_temp.temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value allows us to *reject* a unit root (i.e. the data is stationary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here's an example of non-stationary with DFT results\n",
    "dftest(np.exp(annual_temp.temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "627bd565-118e-4550-8d05-60191a40a0f7"
    }
   },
   "source": [
    "### NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "150671c3-8e05-4d77-acc2-5dcf82bb601d"
    }
   },
   "source": [
    "One of the key concepts in the quantitative toolbox is that of mean reversion. This process refers to a time series that displays a tendency to revert to its historical mean value. Mathematically, such a (continuous) time series is referred to as an Ornstein-Uhlenbeck process. This is in contrast to a random walk (aka Brownian motion), which has no \"memory\" of where it has been at each particular instance of time. The mean-reverting property of a time series can be exploited in order to produce better predictions.\n",
    "\n",
    "A continuous mean-reverting time series can be represented by an Ornstein-Uhlenbeck stochastic differential equation:\n",
    "\n",
    "$dx_{t} = θ(μ−x_{t})dt + σdW_{t}$\n",
    " \n",
    "Where: \n",
    "- θ is the rate of reversion to the mean, \n",
    "- μ is the mean value of the process, \n",
    "- σ is the variance of the process and \n",
    "- $W_{t}$ is a Wiener Process or Brownian Motion.\n",
    "\n",
    "In a discrete setting the equation states that the change of the price series in the next time period is proportional to the difference between the mean price and the current price, with the addition of Gaussian noise.\n",
    "\n",
    "This property motivates the Augmented Dickey-Fuller Test, which we will describe below.\n",
    "\n",
    "https://www.quantstart.com/articles/Basics-of-Statistical-Mean-Reversion-Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "81feb119-d24b-45fc-bb19-12a003a78136"
    }
   },
   "source": [
    "## Section 2: ARIMA with Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a8fc8ecc-2519-407a-973e-28d33ce53b59"
    }
   },
   "source": [
    "Enter [Autoregressive Integrated Moving Average (ARIMA)](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) modeling. When we have autocorrelation between outcomes and their ancestors, we will see a theme, or relationship in the outcome plot. This relationship can be modeled in its own way, allowing us to predict the future with a confidence level commensurate to the strength of the relationship and the proximity to known values (prediction weakens the further out we go).\n",
    "\n",
    "- [ARIMA in R](https://www.otexts.org/fpp/8/5)\n",
    "- [Duke ARIMA Guide](https://people.duke.edu/~rnau/411arim2.htm)\n",
    "- [Great explanation on MA in practice](http://stats.stackexchange.com/questions/164824/moving-average-ma-process-numerical-intuition)\n",
    "\n",
    "### Autoregressive Models\n",
    "\n",
    "*Autocorrelation:* a variable's correlation with itself at different lags.\n",
    "\n",
    "For second-order stationary (both mean and variance: $\\mu_t = \\mu$ and $\\sigma_t^2 = \\sigma^2$ for all $t$) data, autocovariance is expressed as a function only of the time lag $k$:\n",
    "\n",
    "$$ \\gamma_k = E[(x_t-\\mu)(x_{t+k} - \\mu)] $$\n",
    "  \n",
    "Therefore, the autocorrelation function is defined as:\n",
    "\n",
    "$$ \\rho_k = \\frac{\\gamma_k}{\\sigma^2} $$\n",
    "  \n",
    "We use the plot of these values at different lags to determine optimal ARIMA parameters. Notice how `phi` changes the process.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/ArTimeSeries.svg/685px-ArTimeSeries.svg.png)\n",
    "By Tomaschwutz - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=14740378\n",
    "\n",
    "\n",
    "Some things to note:\n",
    "1. AR models propagate shocks infinitely\n",
    "1. Current random error is the `epsilon` error term\n",
    "1. If a process depends on previous values of itself then it is an AR process. If it depends on previous errors than it is an MA process.\n",
    "1. AR processes will exhibit exponential decay in ACR and a cut-off in PACR\n",
    "\n",
    "### Moving Average Models (This is NOT a Simple/Weighted/Exponential Moving Average)\n",
    "\n",
    "Some things to note:\n",
    "1. MA models do not propagate shocks infinitely; they die after `q` lags\n",
    "1. All previous errors up to a lag are rolled into the `epsilon` error term for that period\n",
    "1. If a process depends on previous values of itself, then it is an AR process. If it depends on previous errors, then it is an MA process.\n",
    "1. MA processes will exhibit exponential decay in PACR and a cut-off in ACR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "96e10e23-608c-449f-b96f-8e8279251fd5"
    }
   },
   "source": [
    "### NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d2b7e6f3-c1b6-4fba-a8b4-023e87ff7090"
    }
   },
   "source": [
    "Thus, a moving-average model is conceptually a linear regression of the current value of the series against current and previous (unobserved) white noise error terms or random shocks. The random shocks at each point are assumed to be mutually independent and to come from the same distribution, typically a normal distribution, with location at zero and constant scale.\n",
    "\n",
    "### Interpretation\n",
    "The moving-average model is essentially a finite impulse response filter applied to white noise, with some additional interpretation placed on it. The role of the random shocks in the MA model differs from their role in the autoregressive (AR) model in two ways. First, they are propagated to future values of the time series directly: for example, \n",
    "$\\varepsilon _{t-1}$ appears directly on the right side of the equation for $X_{t}$. In contrast, in an AR model $\\varepsilon _{t-1}$ does not appear on the right side of the $X_{t}$ equation, but it does appear on the right side of the $X_{t-1}$ equation, and $X_{t-1}$ appears on the right side of the $X_{t}$ equation, giving only an indirect effect of $\\varepsilon _{t-1}$ on $X_{t}$. Second, in the MA model a shock affects X values only for the current period and q periods into the future; in contrast, in the AR model a shock affects X values infinitely far into the future, because $\\varepsilon _{t}$ affects $X_{t}$, which affects $X_{t+1}$, etc.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Moving-average_model\n",
    "\n",
    "-----------\n",
    "Consider a series consisting of the closing price (adjusted for splits and dividends) of a stock on consecutive days. Each day's closing price is derived from a trend (e.g., linear in time) plus the weighted effects of the daily shocks from prior days. \n",
    "\n",
    "Presumably, the effect of the shock at day t-1 will have a stronger influence on the price at day t than will the shock at day t-2, etc. Thus, logically, the stock's closing price at day t will reflect the trend value on day t plus a constant (less than 1) times the weighted sum of the shocks up through day t-1 (i.e., the error term at day t-1)(MA1), possibly plus a constant (less than 1) times the weighted sum of the shocks up through day t-2 (i.e., the error term at day t-2)(MA2), ..., plus the novel shock at day t (white noise).\n",
    "\n",
    "This kind of model seems appropriate for modeling series like the stock market, where the error term at day t represents the weighted sum of prior and current shocks, and defines an MA process.\n",
    "\n",
    "https://stats.stackexchange.com/questions/107834/under-what-circumstances-is-an-ma-process-or-ar-process-appropriate\n",
    "\n",
    "Many ways to predict future TS data, but SARIMA is performant. Other methods that we won't go into:\n",
    "- Vector autoregressions (VARs)\n",
    "- Gaussian state space models – often called structural time series or unobserved component models\n",
    "- GARCH\n",
    "- Generalized Autoregressive Score (GAS)\n",
    "- Kalman Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "16e8120d-0ef5-4cfe-9cc6-727a38ad1b4d"
    }
   },
   "source": [
    "### SARIMA\n",
    "![SARIMA Form](https://www.otexts.org/sites/default/files/fpp/images/sarima1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "30edf719-cba2-4c09-9f85-dc5ab1042012"
    }
   },
   "source": [
    "### Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "985b11bd-7d59-4dcf-8729-da9d06ab0078"
    }
   },
   "outputs": [],
   "source": [
    "# define helper plot function for visualization\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "def plots(data, lags=None):\n",
    "    layout = (1, 3)\n",
    "    raw  = plt.subplot2grid(layout, (0, 0))\n",
    "    acf  = plt.subplot2grid(layout, (0, 1))\n",
    "    pacf = plt.subplot2grid(layout, (0, 2))\n",
    "    \n",
    "    data.plot(ax=raw)\n",
    "    smt.graphics.plot_acf(data, lags=lags, ax=acf)\n",
    "    smt.graphics.plot_pacf(data, lags=lags, ax=pacf)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "4b578826-c267-4801-9b16-f10ba8c3e725"
    }
   },
   "outputs": [],
   "source": [
    "# helper plot for monthly temps\n",
    "plots(monthly_temp, lags=75);\n",
    "# open Duke guide for visual\n",
    "# we note a 12-period cycle (yearly) with suspension bridge design, so must use SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b352818a-a4b5-4348-b636-c43a99a142fd"
    }
   },
   "source": [
    "### [Box-Jenkins Method](https://en.wikipedia.org/wiki/Box–Jenkins_method)\n",
    "\n",
    "ACF Shape|Indicated Model\n",
    "---|---\n",
    "Exponential, decaying to zero|Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model.\n",
    "Alternating positive and negative, decaying to zero|Autoregressive model. Use the partial autocorrelation plot to help identify the order.\n",
    "One or more spikes, rest are essentially zero|Moving average model, order identified by where plot becomes zero.\n",
    "Decay, starting after a few lags|Mixed autoregressive and moving average (ARMA) model.\n",
    "All zero or close to zero|Data are essentially random.\n",
    "High values at fixed intervals|Include seasonal autoregressive term.\n",
    "No decay to zero|Series is not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "6ffff0cc-e623-48ee-923c-94cd890ea7bf"
    }
   },
   "outputs": [],
   "source": [
    "# we might need to install dev version for statespace functionality\n",
    "#!pip install git+https://github.com/statsmodels/statsmodels.git\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# fit SARIMA monthly based on helper plots\n",
    "sar = sm.tsa.statespace.SARIMAX(monthly_temp.temp, \n",
    "                                order=(1,1,0), \n",
    "                                seasonal_order=(0,1,0,12), \n",
    "                                trend='c').fit()\n",
    "sar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "83ad04e8-0cd1-4687-aa59-3c109a329f6e"
    }
   },
   "outputs": [],
   "source": [
    "# plot resids\n",
    "plots(sar.resid, lags=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought process:  \n",
    "010010 is overdiff by AIC and negative ACR, but 000010 is a big underdiff with better AIC\n",
    "we pick 000010,12 and Trend='c' per rule 4/5l\n",
    "\n",
    "Now look at seasonal. Notice negative ACR spike at 12: per rule 13, we add a SMA term and we see a big drop to 4284 AIC\n",
    "looks like ACR looks good at seasonal lags, so we move back to ARIMA portion.\n",
    "\n",
    "Rule 6 says we're a bit underdiff, so we add AR=3 based on PACF: 4261 AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2ca50344-0a60-4cf7-b943-97e7c7d42342"
    }
   },
   "outputs": [],
   "source": [
    "# plot residual diagnostics\n",
    "sar.plot_diagnostics();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ad25f01b-d0e5-4db7-ad19-8d100adc0546"
    }
   },
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "monthly_temp['forecast'] = sar.predict(start = 750, end= 790, dynamic=False)  \n",
    "monthly_temp[730:][['temp', 'forecast']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5b280b7-81d2-45ec-9fbf-85d6fe38bb46"
    }
   },
   "source": [
    "## Section 3: Statistical Tests\n",
    "\n",
    "\n",
    "- [Normality (Jarque-Bera)](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_normality.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_normality)\n",
    "    - Null hypothesis is normally distributed residuals (good, plays well with RMSE and similar error metrics)\n",
    "\n",
    "- [Serial correlation (Ljung-Box)](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_serial_correlation.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_serial_correlation)\n",
    "    - Null hypothesis is no serial correlation in residuals (independent of each other)\n",
    "\n",
    "- [Heteroskedasticity](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_heteroskedasticity.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_heteroskedasticity)\n",
    "    - Tests for change in variance between residuals.\n",
    "    - The null hypothesis is no heteroskedasticity. That means different things depending on which alternative is selected:\n",
    "        - Increasing: Null hypothesis is that the variance is not increasing throughout the sample; that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.\n",
    "        - Decreasing: Null hypothesis is that the variance is not decreasing throughout the sample; that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample.\n",
    "        - Two-sided (default): Null hypothesis is that the variance is not changing throughout the sample. Both that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample and that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.\n",
    "\n",
    "- [Durbin Watson](https://en.wikipedia.org/wiki/Durbin–Watson_statistic)\n",
    "    - Tests autocorrelation of residuals: we want between 1-3, 2 is ideal (no serial correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "beffe543-0e64-4f9f-8794-981d5b017e04"
    }
   },
   "outputs": [],
   "source": [
    "# create and run statistical tests on model\n",
    "norm_val, norm_p, skew, kurtosis = sar.test_normality('jarquebera')[0]\n",
    "lb_val, lb_p = sar.test_serial_correlation(method='ljungbox')[0]\n",
    "het_val, het_p = sar.test_heteroskedasticity('breakvar')[0]\n",
    "\n",
    "# we want to look at largest lag for Ljung-Box, so take largest number in series\n",
    "# there's intelligence in the method to determine how many lags back to calculate this stat\n",
    "lb_val = lb_val[-1]\n",
    "lb_p = lb_p[-1]\n",
    "durbin_watson = sm.stats.stattools.durbin_watson(sar.filter_results.standardized_forecasts_error[0, sar.loglikelihood_burn:])\n",
    "\n",
    "print('Normality: val={:.3f}, p={:.3f}'.format(norm_val, norm_p));\n",
    "print('Ljung-Box: val={:.3f}, p={:.3f}'.format(lb_val, lb_p));\n",
    "print('Heteroskedasticity: val={:.3f}, p={:.3f}'.format(het_val, het_p));\n",
    "print('Durbin-Watson: d={:.2f}'.format(durbin_watson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "17940974-3ca2-4e9b-9d36-fed9df31ab0e"
    }
   },
   "source": [
    "### Note on autofit methods\n",
    "R has an autoARIMA function (and other automagic methods) that gridsearches/optimizes our model hyperparameters for us. Over time, more of these goodies are porting to Python (e.g. statsmodels.tsa.x13 import x13_arima_select_order). While there's nothing wrong with utilizing these resources, the _human makes the final determination!_ Don't become over-reliant on these methods, especially early on when you are grasping the underlying mechanics and theory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "fe42022e-9a3b-43aa-b9ee-5a8cb62a67fc"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# autoselect for monthly, limited to only searching AR and MA parameters: UPDATE MAX_AR AND MA\n",
    "autores = sm.tsa.arma_order_select_ic(monthly_temp.temp, \n",
    "                                      ic=['aic', 'bic'], \n",
    "                                      trend='c', \n",
    "                                      max_ar=4, \n",
    "                                      max_ma=4, \n",
    "                                      fit_kw=dict(method='css-mle'))\n",
    "\n",
    "print('AIC', autores.aic_min_order) # will use this as inputs for annual\n",
    "print('BIC', autores.bic_min_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a02122ca-d513-4a2d-b23f-768f63f677a9"
    }
   },
   "outputs": [],
   "source": [
    "# SETUP (using itertools to gridsearch solutions)\n",
    "import itertools\n",
    "\n",
    "#set parameter range; feel free to update these!\n",
    "p = q = range(0, 3)\n",
    "d = range(0, 2)\n",
    "season = 12\n",
    "\n",
    "# list of all parameter combos\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "# same for seasonal variant\n",
    "seasonal_pdq = [(x[0], x[1], x[2], season) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('SARIMAX: {} , {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} , {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} , {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} , {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source:* https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "4bb15437-c6c1-45ab-9741-e497887e4d01"
    }
   },
   "outputs": [],
   "source": [
    "# APPLY (find optimal ARIMA for annual_\n",
    "\n",
    "# UNCOMMENT THE FOLLOWING TO RUN (note: this can take awhile)\n",
    "# warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "# for param in pdq:\n",
    "#     for param_seasonal in seasonal_pdq:\n",
    "#         try:\n",
    "#             mod = sm.tsa.statespace.SARIMAX(annual_temp,\n",
    "#                                             order=param,\n",
    "#                                             seasonal_order=param_seasonal,\n",
    "#                                             enforce_stationarity=False,\n",
    "#                                             enforce_invertibility=False)\n",
    "\n",
    "#             results = mod.fit()\n",
    "\n",
    "#             print('ARIMA{},{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "#         except:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "01592c42-cd74-47f3-a8e5-1e5a6cef0ee0"
    }
   },
   "source": [
    "## Section 4: ARIMA with Pyflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "0e8b941b-0c01-4a04-b23a-805b8598f3d8"
    }
   },
   "outputs": [],
   "source": [
    "# helper plot\n",
    "plots(monthly_temp.temp, lags=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7a4df37a-ec90-4c06-8636-32099daeebff"
    }
   },
   "outputs": [],
   "source": [
    "# build and summarize model\n",
    "model = pf.ARIMA(data=monthly_temp, ar=2, ma=2, integ=0, target='temp')\n",
    "x = model.fit(\"MLE\")\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f4414c1d-612c-4c14-83f9-133adc2bb79d"
    }
   },
   "outputs": [],
   "source": [
    "# plot z-scores of feature coefficients\n",
    "model.plot_z(indices=range(1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "97239903-4d31-430e-b85a-5694867666e9"
    }
   },
   "outputs": [],
   "source": [
    "# plot model against raw data\n",
    "model.plot_fit(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1c6d2b86-47c9-4655-b3bc-c7833aaa126e"
    }
   },
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "model.plot_predict_is(50, fit_once=False, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot forecast\n",
    "model.plot_predict(h=20, past_values=20, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf0fbe23-2778-4730-ac8b-5ebd0adb1334"
    }
   },
   "source": [
    "### Predicting Sunspots with Pyflux and ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "548737a2-0250-46b1-87db-7ad700847678"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# read and plot data\n",
    "data_path = 'https://vincentarelbundock.github.io/Rdatasets/csv/datasets/sunspot.year.csv'\n",
    "data = pd.read_csv(data_path) #\n",
    "data.index = data['time'].values\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data.index,data['value'])\n",
    "plt.ylabel('Sunspots')\n",
    "plt.title('Yearly Sunspot Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "830a86a6-1f1e-4d10-a322-3abf1d68fb95"
    }
   },
   "outputs": [],
   "source": [
    "# fit and summarize model\n",
    "model = pf.ARIMA(data=data,ar=4,ma=4,integ=0,target='value')\n",
    "x = model.fit(\"MLE\")\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a55863df-5810-4945-b8d1-58f458e496f3"
    }
   },
   "outputs": [],
   "source": [
    "# plot z-scores of feature coefficients\n",
    "model.plot_z(indices=range(1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "cfc3de27-f94d-4608-b76b-a3afe27c3e92"
    }
   },
   "outputs": [],
   "source": [
    "# plot model\n",
    "model.plot_fit(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "6347307b-f0c8-4ff1-9cb9-d4e740dd6b12"
    }
   },
   "outputs": [],
   "source": [
    "# plot in sample\n",
    "model.plot_predict_is(50, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7650da03-85fb-445d-bd01-b6e6fc6f92a4"
    }
   },
   "outputs": [],
   "source": [
    "# plot forecast\n",
    "model.plot_predict(h=20,past_values=20, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d2aa26b9-c8d8-41d2-84a7-6d83725d49ad"
    }
   },
   "source": [
    "## Section 5: Statsmodels - CO2 Levels and Forecasting\n",
    "Let's look at sensor data that tracks atmospheric CO2 from continuous air samples at Mauna Loa Observatory in Hawaii. This data includes CO2 samples from MAR 1958 to DEC 2001.\n",
    "\n",
    "[Credits](https://www.digitalocean.com/community/users/tvincent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "75343b23-9dde-48ea-bbb6-26a095cca54b"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "co2 = pd.read_csv('../Data/co2-ppm-mauna-loa-19651980.csv', \n",
    "                  header = 0,\n",
    "                  names = ['idx', 'co2'],\n",
    "                  skipfooter = 2)\n",
    "co2 = co2.drop('idx', 1)\n",
    "\n",
    "# recast co2 col to float\n",
    "co2['co2'] = pd.to_numeric(co2['co2'])\n",
    "co2.drop(labels=0, inplace=True)\n",
    "\n",
    "# set index\n",
    "index = pd.date_range('1/1/1965', periods=191, freq='M')\n",
    "co2.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "fbb37ae5-bccf-4e37-ae3e-2436a09eecd2"
    }
   },
   "outputs": [],
   "source": [
    "# resample to monthly and check missing values\n",
    "co2 = co2['co2'].resample('M').mean()\n",
    "\n",
    "co2 = co2.fillna(co2.bfill())\n",
    "print(\"Nulls:\", co2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1e648b0c-06ef-49b7-af1f-6c25d1f74fb3"
    }
   },
   "outputs": [],
   "source": [
    "# decompose data into trend, seasonal, and residual\n",
    "plt.style.use('fivethirtyeight')\n",
    "decomposition = sm.tsa.seasonal_decompose(co2, model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "62e1967b-d9bd-4236-a941-9f26e20ae46b"
    }
   },
   "outputs": [],
   "source": [
    "# optimize our SARIMAX model using itertools: CAREFUL, this can take a while\n",
    "# warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "# for param in pdq:\n",
    "#     for param_seasonal in seasonal_pdq:\n",
    "#         try:\n",
    "#             model = sm.tsa.statespace.SARIMAX(co2, order=param,\n",
    "#                         seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "\n",
    "#             print('ARIMA{},{} - AIC:{}'.format(param, param_seasonal, model.aic))\n",
    "#         except:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "744fd8bf-59e1-466d-8300-ea1facbeebd5"
    }
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "co2sar = sm.tsa.statespace.SARIMAX(co2, order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 1, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False).fit()\n",
    "co2sar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "32c5f481-050a-4845-be67-20c46ca8fb17"
    }
   },
   "outputs": [],
   "source": [
    "# check diagnostics\n",
    "co2sar.plot_diagnostics();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "dbde99e3-7799-481d-896a-15e08acc05c8"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# create predictions and confidence intervals\n",
    "pred = co2sar.get_prediction(start=pd.to_datetime('1979-4-30'), dynamic=False) # we use as many true values as possible to predict\n",
    "pred_ci = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f2e5555c-5a28-4abe-9255-d42027108b82"
    }
   },
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "ax = co2.plot(label='Observed CO2 Levels')\n",
    "pred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.8) # this is using all available info\n",
    "\n",
    "ax.fill_between(pred_ci.index, pred_ci.iloc[:, 0], pred_ci.iloc[:, 1], color='k', alpha=.1)\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('CO2')\n",
    "plt.legend()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2c3929e8-0377-47fa-9e77-e8a0aca13f5a"
    }
   },
   "outputs": [],
   "source": [
    "# compute mean square error\n",
    "fcast = pred.predicted_mean\n",
    "true = co2['1979-4-30':]\n",
    "\n",
    "mse = ((fcast - true) ** 2).mean()\n",
    "print('MSE of our forecasts is {}'.format(round(mse, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7560422d-62d7-461e-a992-fc2a7836af31"
    }
   },
   "outputs": [],
   "source": [
    "# dynamic forecast\n",
    "fcast = co2sar.get_prediction(start=pd.to_datetime('1979-4-30'), dynamic=True, full_results=True)\n",
    "fcast_ci = fcast.conf_int()\n",
    "\n",
    "# caution: this is modeling assumptions on top of assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcast_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7223c1c5-21be-4ef7-9b94-d58f53c4c148"
    }
   },
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "ax = co2['1979':].plot(label='Observed CO2 Levels')\n",
    "fcast.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(fcast_ci.index, fcast_ci.iloc[:, 0],\n",
    "                fcast_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('1980-11-30'), co2.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('CO2')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "39fdec7c-e3fc-4457-bd36-773e624d1217"
    }
   },
   "outputs": [],
   "source": [
    "# compute mean square error\n",
    "fcast_avg = fcast.predicted_mean\n",
    "true = co2['1979-4-30':]\n",
    "\n",
    "mse = ((fcast_avg - true) ** 2).mean()\n",
    "print('MSE is {}'.format(round(mse, 3)))\n",
    "\n",
    "# notice it's much higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "797ad82f-1fb5-4a2f-bee1-23c89565537c"
    }
   },
   "outputs": [],
   "source": [
    "# forecast next 100 months and get confidence interval\n",
    "pred_uc = co2sar.get_forecast(steps=100)\n",
    "\n",
    "pred_ci = pred_uc.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "361d5676-cbf4-4b14-b907-755ae4b584c1"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot forecast\n",
    "ax = co2[:].plot(label='Observed CO2 Levels')\n",
    "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('CO2')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "51115ac6-e0f0-4502-a4fa-de52a936e5fe"
    }
   },
   "source": [
    "# Section 6: Predicting with [Facebook](https://facebookincubator.github.io/prophet/) [Prophet](https://research.fb.com/prophet-forecasting-at-scale/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ca02a194-adb8-4f0e-ba49-a20a29d1798d"
    }
   },
   "source": [
    "From site:\n",
    "\n",
    "> Today Facebook is open sourcing Prophet, a forecasting tool available in Python and R. Forecasting is a data science task that is central to many activities within an organization. For instance, large organizations like Facebook must engage in capacity planning to efficiently allocate scarce resources and goal setting in order to measure performance relative to a baseline. Producing high quality forecasts is not an easy problem for either machines or for most analysts. We have observed two main themes in the practice of creating a variety of business forecasts:\n",
    "- Completely automatic forecasting techniques can be brittle and they are often too inflexible to incorporate useful assumptions or heuristics.\n",
    "- Analysts who can produce high quality forecasts are quite rare because forecasting is a specialized data science skill requiring substantial experience.\n",
    "\n",
    "Prophet is an additive regression model that includes a number of highly advanced, intelligent [forecasting methods](http://andrewgelman.com/2017/03/01/facebooks-prophet-uses-stan/), including [changepoint analysis](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwjpwM_JhsbVAhWNxIMKHSWqD6kQFgguMAE&url=http%3A%2F%2Fwww.variation.com%2Fcpa%2Ftech%2Fchangepoint.html&usg=AFQjCNFK6wbwWuBCixZJHu03LkABXL3UHA):\n",
    "\n",
    "- A piecewise linear or logistic growth curve trend. Prophet automatically detects changes in trends by selecting changepoints from the data.\n",
    "- A yearly seasonal component modeled using Fourier series.\n",
    "- A weekly seasonal component using dummy variables.\n",
    "- A user-provided list of important holidays.\n",
    "\n",
    "And because it's built in [Stan](http://mc-stan.org/), it's very fast, and funcionality/code translates easily between R and Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "add4561b-f272-4352-b493-f7f854ad1513"
    }
   },
   "source": [
    "Prophet is optimized for the business forecast tasks we have encountered at Facebook, which typically have any of the following characteristics:\n",
    "\n",
    "- Hourly, daily, or weekly observations with at least a few months (preferably a year) of history\n",
    "- Strong multiple “human-scale” seasonalities: day of week and time of year\n",
    "- Important holidays that occur at irregular intervals that are known in advance\n",
    "- A reasonable number of missing observations or large outliers\n",
    "- Historical trend changes, for instance due to product launches or logging changes\n",
    "- Trends that are non-linear growth curves, where a trend hits a natural limit or saturates\n",
    "\n",
    "[Technical details behind prophet](https://facebookincubator.github.io/prophet/static/prophet_paper_20170113.pdf): built around a generalized additive model (GAM)\n",
    "\n",
    "More [GAM background](http://www.stat.cmu.edu/~cshalizi/350/lectures/21/lecture-21.pdf):\n",
    "\n",
    "This includes the linear model as a special case, where fj(xj) = βjxj, but it’s clearly more general, because the fjs can be pretty arbitrary nonlinear functions. The idea is still that each input feature makes a separate contribution to the response, and these just add up, but these contributions don’t have to be strictly proportional to the inputs. We do need to add a restriction to make it identifiable; without loss of generality, say that E [Y ] = α and E [fj (Xj )] = 0.3\n",
    "Additive models keep a lot of the nice properties of linear models, but are more flexible. One of the nice things about linear models is that they are fairly straightforward to interpret: if you want to know how the prediction changes as you change xj, you just need to know βj. The partial response function fj plays the same role in an additive model: of course the change in prediction from changing xj will generally depend on the level xj had before perturbation, but since that’s also true of reality that’s really a feature rather than a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "8a028a7a-a6e9-4804-915d-fd6c37a6962d"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read daily page views for the Wikipedia page for Peyton Manning; scraped into hosted CSV\n",
    "# conda install -c conda-forge fbprophet (to install)\n",
    "from fbprophet import Prophet\n",
    "\n",
    "data_path = 'https://raw.githubusercontent.com/PinkWink/DataScience/master/data/07.%20example_wp_peyton_manning.csv'\n",
    "peyton = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f9f19738-bbd3-4619-8e52-ea341c3be9e7"
    }
   },
   "outputs": [],
   "source": [
    "# plot data\n",
    "peyton.plot()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ee67d09b-da9f-446f-b625-cd6f4b4a73c2"
    }
   },
   "outputs": [],
   "source": [
    "# log data due to spikes\n",
    "peyton['y'] = np.log(peyton['y'])\n",
    "peyton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "47c8f1d9-0612-4fa5-a1f4-c9acccf8d1c6"
    }
   },
   "outputs": [],
   "source": [
    "# plot log\n",
    "peyton.plot()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "baeb2d37-41f7-4b40-939f-a479bc8326d3"
    }
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "m = Prophet()\n",
    "m.fit(peyton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "af976eea-a7c3-4fae-9ac8-f7abaabf29ac"
    }
   },
   "outputs": [],
   "source": [
    "# forecast 365 days into future\n",
    "future = m.make_future_dataframe(periods=365)\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "66558e86-2e7a-4585-ab24-c5c5009e2676"
    }
   },
   "outputs": [],
   "source": [
    "# populate forecast\n",
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "90230a24-4d1d-4bb5-af90-364fdc5dc46b"
    }
   },
   "outputs": [],
   "source": [
    "# plot forecast\n",
    "m.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "e4c6c458-5bde-43ad-848a-16478569f6f8"
    }
   },
   "outputs": [],
   "source": [
    "# plot individual components of forecast: trend, weekly/yearly seasonality,\n",
    "m.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4e554cc3-a53f-4a94-9dd0-0e69e70beef9"
    }
   },
   "source": [
    ">We can also add holiday and Superbowl date information to Peyton's forecast, since we hypothesize people will visit his site more often on those dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b43cd850-f8fa-456a-a445-39686e5c5b8e"
    }
   },
   "outputs": [],
   "source": [
    "# add holidays \n",
    "playoffs = pd.DataFrame({\n",
    "  'holiday': 'playoff',\n",
    "  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n",
    "                        '2010-01-24', '2010-02-07', '2011-01-08',\n",
    "                        '2013-01-12', '2014-01-12', '2014-01-19',\n",
    "                        '2014-02-02', '2015-01-11', '2016-01-17',\n",
    "                        '2016-01-24', '2016-02-07']),\n",
    "  'lower_window': 0, # these help us specify spillover into previous and future days\n",
    "  'upper_window': 1,\n",
    "})\n",
    "\n",
    "superbowls = pd.DataFrame({\n",
    "  'holiday': 'superbowl',\n",
    "  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "\n",
    "holidays = pd.concat((playoffs, superbowls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b0f87b06-75cf-433f-9820-109cb5939753"
    }
   },
   "outputs": [],
   "source": [
    "# fit and predict\n",
    "m = Prophet(holidays=holidays)\n",
    "forecast = m.fit(peyton).predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c565849d-43a8-4453-a4f5-8ce8065756a9"
    }
   },
   "outputs": [],
   "source": [
    "# we can see the effects of various 'holidays' on site visits\n",
    "forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][\n",
    "        ['ds', 'playoff', 'superbowl']][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f728ec7e-bbe1-4963-9f2c-8b9ca84a46de"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check the impacts visually\n",
    "m.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a283c343-2c68-48a5-9276-16a9345be058"
    }
   },
   "source": [
    ">Peyton won Superbowls XLI (41, 2007) and 50 (2016), while losing XLIV (44, 2010) and XLVIII(48, 2014). We can see these spikes in the holidays chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e3e01320-3a45-487f-a826-86757be7cae4"
    }
   },
   "source": [
    "### Predicting CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "co2 = pd.read_csv('../Data/co2-ppm-mauna-loa-19651980.csv', \n",
    "                  header = 0,\n",
    "                  names = ['idx', 'co2'],\n",
    "                  skipfooter = 2)\n",
    "co2 = co2.drop('idx', 1)\n",
    "\n",
    "# recast co2 col to float\n",
    "co2['co2'] = pd.to_numeric(co2['co2'])\n",
    "co2.drop(labels=0, inplace=True)\n",
    "\n",
    "# set index\n",
    "index = pd.date_range('1/1/1965', periods=191, freq='M')\n",
    "co2.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "4ef59c0f-6949-43bb-aa69-88f4d1755214"
    }
   },
   "outputs": [],
   "source": [
    "# load co2 data, rename headers, and check\n",
    "# data = sm.datasets.co2.load_pandas()\n",
    "# co2 = data.data\n",
    "\n",
    "co2['ds'] = co2.index\n",
    "co2.rename(columns={'co2': 'y'}, inplace=True)\n",
    "\n",
    "co2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "cc7e92ae-4752-432a-b2be-2e676a4113e2"
    }
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model = Prophet()\n",
    "model.fit(co2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b2097b62-1a95-4286-86bb-1c60dcc3913d"
    }
   },
   "outputs": [],
   "source": [
    "# forecast 15 years into future\n",
    "future = model.make_future_dataframe(periods=120, freq='M', include_history=True)\n",
    "future.tail()\n",
    "\n",
    "#future = model.make_future_dataframe(periods=365*15)\n",
    "#future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "e24731c2-c36c-4d81-86d1-8572d93b9c1d"
    }
   },
   "outputs": [],
   "source": [
    "# populate forecast\n",
    "forecast = model.predict(future)\n",
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "08e618cb-91fb-4652-aca8-c7cef4b999d6"
    }
   },
   "outputs": [],
   "source": [
    "model.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "cbf53b27-bc78-4697-9a02-5de5a5320980"
    }
   },
   "outputs": [],
   "source": [
    "# plot individual components of forecast: trend, weekly/yearly seasonality,\n",
    "model.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "edf3c59f-0d6e-41bd-ad17-c390d6a5060e"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can add a cap to limit our theoretical growth\n",
    "co2['cap'] = 380\n",
    "m = Prophet(growth='logistic')\n",
    "m.fit(co2);\n",
    "\n",
    "# forecast 15 years into future with cap of 380\n",
    "future = m.make_future_dataframe(periods=365*15, include_history=False)\n",
    "future['cap'] = 380\n",
    "\n",
    "forecast = m.predict(future)\n",
    "m.plot(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0cde930b-ac3c-4032-9dc0-261c37a52306"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2f8f0b11-e363-4227-8803-df1cba7c5f93"
    }
   },
   "source": [
    "# Summary\n",
    "In this notebook you should have gained: \n",
    "1. A practical understanding of Autoregressive Moving Average (ARMA) models.\n",
    "2. A basic understanding of the Autocorrelation Function (ACF).\n",
    "3. Insight into choosing the order *q* of MA models.\n",
    "4. A practical understanding of Autoregressive (AR) models.\n",
    "5. A basic understanding of the Partial Autocorrelation Function (PACF).\n",
    "6. Insight into choosing the order *p* of AR models.\n",
    "\n",
    "Congratulations, that concludes this lesson."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (intel_timeseries)",
   "language": "python",
   "name": "intel_timeseries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": false,
   "nav_menu": {
    "height": "311px",
    "width": "412px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": false,
   "toc_position": {
    "height": "22px",
    "left": "1105px",
    "right": "20px",
    "top": "-1px",
    "width": "22px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the previous three lessons, you learned several fundamental time series concepts like stationarity and smoothing. Now we'll build upon that knowledge by digging into yet another important concept called **autocorrelation**.\n",
    "\n",
    "# Learning Outcomes\n",
    "You should walk away from this tutorial with:\n",
    "1. A practical understanding of Moving Average (MA) models.\n",
    "2. A basic understanding of the Autocorrelation Function (ACF).\n",
    "3. Insight into choosing the order *q* of MA models.\n",
    "4. A practical understanding of Autoregressive (AR) models.\n",
    "5. A basic understanding of the Partial Autocorrelation Function (PACF).\n",
    "6. Insight into choosing the order *p* of AR models.\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import statsmodels as ss\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Library Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "versions = ( (\"matplotlib\", matplotlib.__version__),\n",
    "            (\"numpy\", np.__version__),\n",
    "            (\"pandas\", pd.__version__),\n",
    "            (\"statsmodels\", ss.__version__) )\n",
    "\n",
    "print(sys.version, \"\\n\")\n",
    "print(\"library\" + \" \" * 4 + \"version\")\n",
    "print(\"-\" * 18)\n",
    "\n",
    "for tup1, tup2 in versions:\n",
    "    print(\"{:11} {}\".format(tup1, tup2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Moving Average (MA) & MA Models \n",
    "\n",
    "In this lesson, we will revisit moving average smoothing and explore the idea of moving average models. Please be aware that these two concepts are not equivalent. Each serves a different, important function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Moving Average?\n",
    "Moving average is perhaps one of the most obvious and simplest techniques you can use to capture local changes in time series data as well as provide forecasts. In fact, we explored MA in some detail last lesson. We discussed simple and exponential smoothing. We also discussed single, double, and triple exponential smoothing. If you're fuzzy about any of those terms, please go back and review now. \n",
    "\n",
    "Let's build upon this knowledge by going a bit deeper. \n",
    "\n",
    "Recall that the forecast value $\\hat{y}_{t+1} = \\frac{y_t + y_{t-1} + ... + y_{t-m+1}}{m}$\n",
    "\n",
    "It's worth pondering that formula for a minute. While easy to understand, one of its properties may not be obvious. What's the lag associated with this technique? Think it through. \n",
    "\n",
    "The answer is $\\frac{(m+1)}{2}$. For example, say you're averaging the past 5 values to make the next prediction. Then local changes will yield a lag of $\\frac{5+1}{2} = 3$ periods. Clearly, the lag increases as you increase the the window size for averaging. \n",
    "\n",
    "You may think to yourself, \"Well, I'll just use a small window size.\" Doing so will create a more responsive model. However, a window size that's too small will chase noise in the data as opposed to extracting the pattern. As with most things in machine learning, there is a tradeoff between the two. The best answer lies somewhere between and requires careful tuning to determine which setup is best for a given dataset. \n",
    "\n",
    "Let's generate some toy data and apply a moving average with different window sizes to see that what we discussed is in fact true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# time component\n",
    "time = np.arange(100)\n",
    "\n",
    "# data\n",
    "noise = np.random.normal(loc=0, scale=6.5, size=len(time))\n",
    "trend = time * 2.75\n",
    "seasonality = 10 + np.sin(time * 0.25) * 20\n",
    "data = trend + seasonality + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another friendly reminder that you should always visually inspect your time series with a run-sequence plot. \n",
    "> We'll leverage the *run_sequence_plot* function to make plotting throughout this tutorial less cumbersome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_sequence_plot(x, y, title, xlabel=\"time\", ylabel=\"series\"):\n",
    "    plt.plot(x, y, 'k-', label=\"actual\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_sequence_plot(time, data, title=\"TS Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we need a moving average function. Thankfully, pandas has a [rolling mean](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.rolling_mean.html) function we can leverage.\n",
    "\n",
    "We'll try three window sizes: 3, 5, 9. Then we'll plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series = pd.Series(data)\n",
    "\n",
    "# case with lag of 2\n",
    "lag_2 = series.rolling(window=3).mean()\n",
    "\n",
    "# case with lag of 3\n",
    "lag_3 = series.rolling(window=5).mean()\n",
    "\n",
    "# case with lag of 5\n",
    "lag_5 = series.rolling(window=9).mean()\n",
    "\n",
    "# case with lag of 10\n",
    "lag_10 = series.rolling(window=19).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_sequence_plot(time, data, title=\"MA w/Lag 2\")\n",
    "plt.plot(time, lag_2, 'b-', label=\"lag-2\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_sequence_plot(time, data, title=\"MA w/Lag 3\")\n",
    "plt.plot(time, lag_3, 'b-', label=\"lag-3\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_sequence_plot(time, data, title=\"MA w/Lag 5\")\n",
    "plt.plot(time, lag_5, 'b-', label=\"lag-5\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_sequence_plot(time, data, title=\"MA w/Lag 10\")\n",
    "plt.plot(time, lag_10, 'b-', label=\"lag-10\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tradeoff is responsiveness versus smoothing capabilities. Let's make this clearer by plotting the three moving average curves sans actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, sharex=True, sharey=True)\n",
    "axes[0].plot(time, lag_2, 'k-', label=\"lag-2\")\n",
    "axes[0].legend()\n",
    "axes[1].plot(time, lag_3, 'b-', label=\"lag-3\")\n",
    "axes[1].legend()\n",
    "axes[2].plot(time, lag_5, 'r-', label=\"lag-5\")\n",
    "axes[2].legend()\n",
    "axes[3].plot(time, lag_10, 'g-', label=\"lag-10\")\n",
    "axes[3].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA Models != Moving Average Smoothing\n",
    "An important distinction is that a moving average model is not the same thing as moving average smoothing. What we did above was smoothing. It has important properties already discussed. However, we turn our attention to moving average models, which are a completely different beast.\n",
    "\n",
    "Moving average smoothing is useful for estimating trend and seasonality of past data. MA models, on the other hand, are a useful forecasting model that regresses on past forecast errors in order to forecast future values. It is easy to lump the two techniques together, but they serve very different functions. \n",
    "\n",
    "### MA Model Specifics\n",
    "A MA model is defined by this equation: $y_t=c+e_t+θ_1e_{t−1}+θ_2e_{t−2}+⋯+θ_qe_{t−q}$ where $e_t$ is white noise. The value $c$ is a constant value and the $\\theta$'s are coefficients, not unlike those found in linear regression.\n",
    "\n",
    "Fitting a MA models is quite involved and requires iterative procedures. The details are beyond the scope of this lesson. However, statsmodels has this functionality built in so we can build a model with minimal effort. Let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA Models with Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot different orders (q's)\n",
    "for i in range(1,6):\n",
    "    model = ARMA(data, (0,i)).fit()\n",
    "    run_sequence_plot(time, data, title=\"Statsmodels MA (q={})\".format(i))\n",
    "    plt.plot(time, model.predict(start=1, end=100), label=\"MA model\")\n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the basics of MA models, let's turn our attention to selecting the proper order called *q*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: The Autocorrelation Function (ACF)\n",
    "There's a crucial question we need to answer: How do you choose the order, known as *q*, of the MA model?\n",
    "\n",
    "In order to answer that question, we need to understand the Autocorrelation Function (ACF). \n",
    "\n",
    "Let's start by showing an example ACF plot for our dataset called *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "fig = plot_acf(data, lags=range(1,15), alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation is in order. First, the blue region represents a confidence interval. Alpha in this case was set to 0.05 (95% confidence interval). This can be set to whatever float value you require. See the **plot_acf** function for details. \n",
    "\n",
    "The stems represent lagged correlation values. In other words, a lag of 1 correlates almost perfectly with the current endogenous value. A lag of 2 correlates nearly but not quite as well. And so on. Remember that we're regressing on past forecast errors; that's the correlation we're inspecting here. \n",
    "\n",
    "Correlations outside of the confidence interval are statistically significant whereas the others are not. You can use this plot to choose the value of *q* to include in your MA model. One key thing to note is that **MA models require stationarity**. You should see correlations drop quickly. If they don't, that's a sign that your data is not stationary, as we see here.\n",
    "\n",
    "In the interest of rigor, we'll test our data with the Augmented Dickey-Fuller test, though we know how this particular data was generated (i.e. it has trend and seasonality). Therefore, it is not stationary. We can, however, transform it by applying a difference and checking for stationarity once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(data)\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A p-value that high means we must fail to reject the null that this data is nonstationary - as we knew all along. Now let's apply three differences and check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "difference = data[:-1] - data[1:]\n",
    "difference2 = difference[:-1] - difference[1:]\n",
    "\n",
    "adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(difference2)\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're stationary now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_sequence_plot(time[2:], difference2, title=\"Stationary Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plot_acf(difference2, lags=range(1,15), alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows AR(1) is most appropriate.\n",
    "> See [here](https://people.duke.edu/~rnau/411arim2.htm) for an in-depth explanation.\n",
    "\n",
    "That concludes our discussion of MA models. Let's now turn our attention to AR models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Autoregressive (AR) Models\n",
    "In this lesson, we will explore the idea of autoregressive models and how to determine the proper order with the Partial Autocorrelation Function (PACF). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an Autoregressive Model?\n",
    "AR models are similar to MA models with one key distinction. MA models regress on past forecast errors in order to forecast future values. On the other hand, AR models regress on actual past values, not past errors. \n",
    "\n",
    "This is the first order or **AR(1)** formula you should know: $y_t = \\beta_0 + \\beta_1y_{t-1}+\\epsilon_t$\n",
    "\n",
    "The $\\beta$'s are just like those in linear regression and $\\epsilon$ is irreducible error.\n",
    "\n",
    "A second order or **AR(2)** would look like this: $y_t = \\beta_0 + \\beta_1y_{t-1}+\\beta_2y_{t-2}+\\epsilon_t$\n",
    "\n",
    "The pattern of adding another coefficient and another past term continues to whichever order you choose. Choosing that order called *p* is something we will discuss shortly.\n",
    "\n",
    "In the meantime, let's walk through a basic autocorrelation example. We'll generate our own data so we know the generative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed to start series\n",
    "seed = 14\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# create autocorrelated data\n",
    "lagged = np.empty_like(time, dtype='float')\n",
    "for t in time:\n",
    "    lagged[t] = seed + np.random.normal(loc=0, scale=2.5, size=1)\n",
    "    seed = lagged[t]\n",
    "    if t > 0:\n",
    "        lagged[t] = lagged[t] + (0.7 * lagged[t-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_sequence_plot(time, lagged, title=\"Autocorrelated Data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot different orders (q's)\n",
    "for i in range(1,6):\n",
    "    model = ARMA(data, (i,0)).fit()\n",
    "    run_sequence_plot(time, data, title=\"Statsmodels AR (p={})\".format(i))\n",
    "    plt.plot(time, model.predict(start=1, end=100), label=\"AR model\")\n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: The Partial Autocorrelation Function (PACF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the order *p* of the AR model is important. Thankfully, there's a useful plot called the Partial Autocorrelation Function plot that can help us with that task. \n",
    "\n",
    "Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "fig = plot_pacf(data, lags=range(1,15), alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is quite easy to read. First off, the blue region once again is the 95% confidence interval. The ACF is a way to measure the linear relationship between a current observation and observations at previous time periods. It turns out we are really only interested in the relationship between the current observation and a past value determined by a lag value. We often don't care about the relationship of values between the two, so we transform them to obtain the PACF. The key takeaway is the PACF is a useful tool for identifying the order of the AR model.\n",
    "\n",
    "This example shows that an AR(1) model is most appropriate for this data. We know this because the partial autocorrelation is significant for this lag but for no other lags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4\n",
    "You have been provided two datasets: \n",
    "1. **auto_1.npy**\n",
    "2. **auto_2.npy**\n",
    "\n",
    "Your task is to leverage what you've learned in this and previous courses. \n",
    "\n",
    "More specifically, you will do the following:\n",
    "1. Read in **auto_1.npy** and **auto_2.npy**.\n",
    "2. Create a time variable called **mytime** that starts at 0 and is as long as both datasets.\n",
    "3. Generate run-sequence plots of auto_1 and auto_2.\n",
    "4. Determine the order of p and q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "path_to_file = \"../Data/\"\n",
    "auto_1 = np.load(path_to_file + \"auto_1.npy\")\n",
    "auto_2 = np.load(path_to_file + \"auto_2.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create mytime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run-Sequence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Determine Order (p & q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this assignment you should have learned the basics of: \n",
    "1. Moving Average (MA) models.\n",
    "2. The Autocorrelation Function (ACF).\n",
    "3. Choosing order *q*.\n",
    "4. Autoregressive (AR) models.\n",
    "5. The Partial Autocorrelation Function (PACF).\n",
    "6. Choosing order *p*. \n",
    "\n",
    "Congratulations, that concludes this lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 OpenCV3 (Forge)",
   "language": "python",
   "name": "opencv-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

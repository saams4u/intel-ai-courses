{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8\n",
    "\n",
    "### LSTM Network for Regression\n",
    "\n",
    "Here we will develop an LSTM networks in Python using the Keras deep learning library to demonstrate a time-series prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some of the libraries, functions and classes we are going to use.  This assumes a working SciPy environment with the Keras deep learning library installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to fix the random number seed to ensure our results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is $CO_2$ production in parts per million (ppm) from the volcano Mauna Loa (https://en.wikipedia.org/wiki/Mauna_Loa) between 1965-1980 by month. Data can be found here: https://datamarket.com/data/set/22v1/co2-ppm-mauna-loa-1965-1980#!ds=22v1&display=line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the dataset using Pandas.  We are not interested in the date, given that each observation is separated by the same interval of one month.  Therefore, when we load the dataset we can exclude the first column.\n",
    "\n",
    "The csv file has footer information that can be excluded with the skipfooter argument to pandas.read_csv() set to 3 for the 3 footer lines.  Once loaded we can visualize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('Data/Lesson8/co2-ppm-mauna-loa-19651980.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "data = df.values\n",
    "data = data.astype('float32')\n",
    "print('Number of datapoints:',len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data.  This data set has an upward trend over time as well as some periodicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "plt.xlabel('TimePoint in Months')\n",
    "plt.ylabel('$CO_2$(ppm)')\n",
    "plt.grid()\n",
    "plt.plot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With time series data, the sequence of the data is important. To create a train-test split we will use the first part of the sequence for training, and the latter part for testing.  The code below calculates the index of the split point and separates the data into the training set with 67% of the observations, and a training set with the remaining 33% of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(data) * 0.67)\n",
    "test_size = len(data) - train_size\n",
    "train, test = data[0:train_size,:], data[train_size:len(data),:]\n",
    "print('Train Set contains:', len(train),'datapoints')\n",
    "print('Test Set contains:', len(test),'datapoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It is recommended to rescale (normalize) the data to the range of [0, 1]. This can be achieved using the MinMaxScaler preprocessing class from the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "Use the MinMaxScaler to normalize the training and testing set betwee 0 and 1.  Name the normalized outputs norm_train and norm_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "plt.xlabel('TimePoint in Months')\n",
    "plt.ylabel('Normalized $CO_2$(ppm)')\n",
    "plt.grid()\n",
    "plt.plot(range(len(train)),norm_train,'b')\n",
    "plt.plot(range(len(train),len(train)+len(test)),norm_test,'r')\n",
    "plt.legend(['Norm Train','Norm Test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "We can write a function that separates the data into an input array (X) which has all the datapoint at times t-i's, and an output array which contains the data at timepoint t.\n",
    "\n",
    "The function takes two arguments: the data, and the look_back, which is the number of previous time steps to use as input variables to predict the next time period â€” the default is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_X_y(data, look_back = 1):\n",
    "# add code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape into X contains the data for times t-i ... t-2 t-1 and Y contains the data for times t\n",
    "look_back = 1 # here we will only look at the previous datapoint to predict the next one\n",
    "trainX, trainY = split_X_y(norm_train, look_back)\n",
    "testX, testY = split_X_y(norm_test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM network expects the input data (X) to be provided in the form of: [sample, time steps, features].\n",
    "\n",
    "Currently, the data is in the form: [samples, features], and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now design and fit our LSTM network.\n",
    "\n",
    "The network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons, and an output layer that makes a single value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 20 epochs and a batch size of 1 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is fit, we can estimate the performance of the model on both the training and testing datasets. \n",
    "\n",
    "Note that we must invert (un-normalize) the predictions before calculating error scores to ensure that performance is reported in the same units as the original data (ppm per month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainYTrue = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testYTrue = scaler.inverse_transform(testY)\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainYTrue, trainPredict))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testYTrue, testPredict))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions have been generates using the model for both the train and test dataset. We can also visualize the results to have an indication for how the model works.\n",
    "\n",
    "When plotting the data, we must shift the predictions by the look_back in time to align on the x-axis with the original dataset. The data is presented with the original dataset as black dots, the predictions for the training dataset in blue, and the predictions on the test dataset in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "plt.xlabel('TimePoint in Months')\n",
    "plt.ylabel('$CO_2$(ppm)')\n",
    "plt.grid()\n",
    "plt.plot(range(len(data)),data,'k.')\n",
    "plt.plot(range(look_back,len(trainPredict)+look_back),trainPredict,'b')\n",
    "plt.plot(range(len(trainPredict)+2*look_back,len(trainPredict)+len(testPredict)+2*look_back),testPredict,'r')\n",
    "plt.legend(['Actual','Train','Test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Improve the results above by trying the following:\n",
    "- Increase the number of epochs\n",
    "- Increase the look_back value\n",
    "- Modify the Architecture\n",
    "- Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 OpenCV3 (Forge)",
   "language": "python",
   "name": "opencv-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

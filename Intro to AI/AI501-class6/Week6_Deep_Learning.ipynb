{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T14:45:06.527670Z",
     "start_time": "2018-02-26T09:45:06.471568-05:00"
    }
   },
   "source": [
    "# Deep Learning Applications\n",
    "\n",
    "This lesson assumes students are \n",
    "- Strong on Data Science fundamentals\n",
    "- Familiar with Deep Learning theory\n",
    "- Comfortable with Sci-Kit Learn\n",
    "\n",
    "At the end of this lesson you should be able to\n",
    "- Use Keras to build models that tackle many common applications\n",
    "- Modify existing \"cookie-cutter\" models to tailor them for specific tasks\n",
    "- Interact with Keras and Tensorflow where needed\n",
    "\n",
    "We can't cover everything so keep these resources handy and refer back to them when you need more details\n",
    "- [Keras documentation](https://keras.io/)\n",
    "- [Tensorflow documentation](https://www.tensorflow.org/api_docs/)\n",
    "- [Official Keras examples](https://github.com/keras-team/keras/tree/master/examples) dozens of examples. just about everything you could need\n",
    "- [Deep Learning Book](http://www.deeplearningbook.org/) Free and just about the best coverage of the theory behind Deep Learning around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:09.867842Z",
     "start_time": "2018-03-02T22:04:07.935835Z"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd \n",
    "import keras\n",
    "\n",
    "IMSIZE = [224,224]\n",
    "BATCH = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T14:47:27.290073Z",
     "start_time": "2018-02-26T09:47:27.258336-05:00"
    }
   },
   "source": [
    "# Deep Learning approaches\n",
    "\n",
    "First, recall from previous lessons the essential parts of any neural network\n",
    "\n",
    "- A chain of differentiable functions that\n",
    "  - Includes trainable parameters.\n",
    "  - Could, with the right parameters, perform the operation you're looking for.\n",
    "  - Includes [non-linearities](https://stackoverflow.com/questions/9782071/why-must-a-nonlinear-activation-function-be-used-in-a-backpropagation-neural-net).\n",
    "- A [loss function](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) that puts a number on \"does the network do what I want it to?\".\n",
    "  - Regression (guess the right number): Mean Squared Error, Mean Average Error\n",
    "  - Categorization (guess the right class): Categorical Cross-Entropy\n",
    "  - Multi-Label classification (guess which labels apply): Binary Cross-Entropy\n",
    "\n",
    "Given these two pieces, we use backpropogation to optimize our trainable function to produce better and better output at each step. \n",
    "\n",
    "\n",
    "We will learn two methods for accomplishing this:\n",
    "\n",
    "## 1 The Tensorflow method\n",
    "\n",
    "Here, we will build the \"function\" that our model computes piece-by-piece. We'll create every variable, define all the operations that connect them, and \"manually\" feed data into the function each time.\n",
    "\n",
    "Here we'll accomplish two things. First, we'll take some of the mystery out of neural nets. This is meant to be a short learning experience. If you wish to include neural nets in your project **it is not recommended to start with this method**. In general, this is an approach you may wish to revisit if, after using the second method, you wish to have more direct control over how your model is built.\n",
    "\n",
    "## 2 The Keras method \n",
    "\n",
    "Here, we use high-level building blocks to quickly compose complex-yet-powerful networks. Keras gives you access to some of the most advanced, difficult to implement architectures without requiring you to implement them by hand. **This is where most neural net projects should start**.\n",
    "\n",
    "Again, just for complete clarity, these methods are taught in the current order so that, when we arrive the Keras approach, we understand what Keras is doing under the hood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T17:14:26.726791Z",
     "start_time": "2018-03-01T17:14:26.657278Z"
    }
   },
   "source": [
    "## Data prep\n",
    "\n",
    "We will focus on image recognition, a task Deep Learning models have excelled in in recent years. We will use data from the [Dogs vs Cats](https://www.kaggle.com/c/dogs-vs-cats/data) Kaggle competition. Follow the link above and download the `train.zip` file under data. Unzip the file and note the location below. \n",
    "\n",
    "First we will pull out 1,000 images each for our train, validation, and test sets. This is necessary to execute these models on most laptop computers. If you have access to a more powerful computer or are happy to wait, feel free to adjust the size of the sets.\n",
    "\n",
    "**TODO:** In the below cell, set the variable dvc_path to the location where you downloaded the dataset. Then run the following cells to create the train, validation, and test set and to visualize a sample of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:09.874983Z",
     "start_time": "2018-03-02T22:04:09.871086Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dvc_path = \"/home/zephyrie/AI_501\"\n",
    "split_sizes = {\"train\": 1000, \"validation\":1000, \"test\":1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:09.880983Z",
     "start_time": "2018-03-02T22:04:09.877657Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, shutil, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:10.604541Z",
     "start_time": "2018-03-02T22:04:09.883694Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_files = set(glob.glob(dvc_path+\"/train/**.jpg\", recursive=True))\n",
    "cat_files = {f for f in labeled_files if \"cat\" in f}\n",
    "dog_files = labeled_files - cat_files\n",
    "\n",
    "print(f\"found {len(labeled_files)} labeled files, {len(cat_files)} of cats and {len(dog_files)} of dogs\")\n",
    "\n",
    "shutil.rmtree(dvc_path+\"/split\", ignore_errors=True)\n",
    "\n",
    "for split in split_sizes:\n",
    "    print(f\"moving {split_sizes[split]} files for {split}\")\n",
    "    n = split_sizes[split]//2\n",
    "    os.makedirs(f\"{dvc_path}/split/{split}/cat\", exist_ok=True)\n",
    "    os.makedirs(f\"{dvc_path}/split/{split}/dog\", exist_ok=True)\n",
    "    \n",
    "    new_cats = set(np.random.choice(list(cat_files), size=n, replace=False))\n",
    "    for new_cat_file in new_cats:\n",
    "        _, fname = new_cat_file.split(\"train/\")\n",
    "        shutil.copy(new_cat_file, f\"{dvc_path}/split/{split}/cat/{fname}\")\n",
    "    cat_files = cat_files - new_cats\n",
    "        \n",
    "    new_dogs = set(np.random.choice(list(dog_files), size=n, replace=False))\n",
    "    for new_dog_file in new_dogs:\n",
    "        _, fname = new_dog_file.split(\"train/\")\n",
    "        shutil.copy(new_dog_file, f\"{dvc_path}/split/{split}/dog/{fname}\")\n",
    "    dog_files = dog_files - new_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:10.614665Z",
     "start_time": "2018-03-02T22:04:10.606525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:10.649118Z",
     "start_time": "2018-03-02T22:04:10.617555Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(x, y, **kwargs):\n",
    "    n_pix = int(np.sqrt(np.prod(x.shape[1:3]))) #assumes images are square\n",
    "    im_indices = np.random.choice(x.shape[0], 36, replace=False)\n",
    "    fig, axes = subplots(nrows=6,ncols=6, figsize=(10,10), sharex=True, sharey=True, frameon=False)\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        curr_i = im_indices[i]\n",
    "        ax.imshow(x[curr_i].reshape(n_pix,n_pix, 3), aspect=\"auto\", **kwargs)\n",
    "        if y[curr_i]==0:\n",
    "            ax.text(10,20,'cat', fontdict={\"backgroundcolor\": \"gray\",\"color\": \"white\" })\n",
    "        else:\n",
    "            ax.text(10,20,'dog', fontdict={\"backgroundcolor\": \"gray\",\"color\": \"white\" })\n",
    "#         ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(h_pad=0, w_pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:14.026172Z",
     "start_time": "2018-03-02T22:04:10.651366Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_gen = datagen.flow_from_directory(\n",
    "    directory=dvc_path+\"/split/train\",\n",
    "    target_size=IMSIZE,\n",
    "    batch_size=36,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "x,y = next(plot_gen)\n",
    "\n",
    "plot_images(x, y, interpolation=\"spline16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T14:47:57.458109Z",
     "start_time": "2018-02-26T09:47:57.451253-05:00"
    }
   },
   "source": [
    "# The Tensorflow method: Deep Learning the hard(er) way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:14.392132Z",
     "start_time": "2018-03-02T22:04:14.028215Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import datasets, model_selection, preprocessing\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:14.609373Z",
     "start_time": "2018-03-02T22:04:14.395011Z"
    }
   },
   "outputs": [],
   "source": [
    "TF_IMSIZE = [50,50]\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1/255, samplewise_center=True, samplewise_std_normalization=True)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    directory=dvc_path+\"/split/train\",\n",
    "    target_size=TF_IMSIZE,\n",
    "    batch_size=BATCH,\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    directory=dvc_path+\"/split/validation\",\n",
    "    target_size=TF_IMSIZE,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:15.172824Z",
     "start_time": "2018-03-02T22:04:14.611433Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 64 # 1st layer number of neurons\n",
    "n_hidden_2 = 32 # 2nd layer number of neurons\n",
    "num_input = np.prod(TF_IMSIZE)*3 #\n",
    "num_classes = 2 # \n",
    "\n",
    "# Placeholders are not trainable (this is our input and output)\n",
    "X = tf.placeholder(\"float\", [None] + TF_IMSIZE + [3])\n",
    "X_flat = tf.contrib.layers.flatten(X)\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables **are** trainable. Below, we define all the varaibles we'll use.\n",
    "\n",
    "Pay attention here to how the dot product affects dimensions. I'll use the following notation to indicate that $A$ is an array with 100 elements along the first dimension and 50 along the second. In other words `A.shape == (100, 50)`.\n",
    "\n",
    "$ A_{[100 \\times 50]} \\cdot B = C $\n",
    "\n",
    "We need to line up the dimensions of our variables such that the last dimension of $A$ matches the first dimension of $B$. Also, keep in mind that $C$, the output is going to have a shape based on the first dimension of $A$ and the last dimension of $B$.\n",
    "\n",
    "$ A_{[100 \\times 50]} \\cdot B_{[50 \\times 25]} = C_{[100 \\times 25]} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:15.222572Z",
     "start_time": "2018-03-02T22:04:15.174995Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placing them in a dictionary is helpful for keeping organized\n",
    "# but these are just python variables.\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:15.241321Z",
     "start_time": "2018-03-02T22:04:15.225983Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the operations we'll use to construct\n",
    "# the output from our inputs and trainable parameters\n",
    "\n",
    "# First hidden fully connected layer\n",
    "layer_1 = tf.matmul(X_flat, weights['h1']) + biases['b1']\n",
    "\n",
    "# Second hidden fully connected layer\n",
    "layer_2 = tf.matmul(layer_1, weights['h2']) + biases['b2']\n",
    "\n",
    "# Output fully connected layer with a neuron for each class\n",
    "logits = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "# Is something missing here? What?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:04:15.359080Z",
     "start_time": "2018-03-02T22:04:15.244414Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "# Define the loss and optimizer\n",
    "# recall that cross-entropy loss is what we use for most categorization problems\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y)\n",
    "loss_op = tf.reduce_mean(loss)\n",
    "\n",
    "# The optimizer uses gradient descent and the backprop algorithm\n",
    "# Most of these are just variations on Stochastic Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:05:44.589965Z",
     "start_time": "2018-03-02T22:04:15.362203Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_steps = 350 # roughly 10 epochs\n",
    "display_step = int(num_steps//20)\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = next(train_gen)\n",
    "        # Run optimization op (backprop)\n",
    "        _, train_loss, train_acc = sess.run([train_op, loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "        loss.append(train_loss)\n",
    "        acc.append(train_acc)\n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            x_val, y_val = next(val_gen)\n",
    "            val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: x_val,\n",
    "                                                                 Y: y_val})\n",
    "            print(f\"Step {step}, Train: Loss={np.mean(loss):.4f}, Acc={np.mean(acc):.2%}\"\n",
    "                  f\"| Val: Loss={val_loss:.4f}, Acc={val_acc:.2%}\")\n",
    "            loss = []\n",
    "            acc = []\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 1**: \n",
    "\n",
    "Explain what is happening here. Specifically, is anything about this network and its performance not ideal? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Problem 2:**\n",
    "\n",
    "Using the above code as a starting point, try to address the problems mentioned above by keeping the two hidden layer structure but adding a non-linearity and re-training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras implementation\n",
    "\n",
    "Keras is a framework for building tensorflow code. It automates many tensorflow tasks that are often repeated and allows data scientists to rapidly build and modify complex networks.\n",
    "\n",
    "The tensorflow model we just built is re-implemented below in tensorflow code. Notice how few lines of code we need. In particular, the messy system for running batches of data through a tensorflow session is very clean here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:06:16.336072Z",
     "start_time": "2018-03-02T22:06:16.118256Z"
    }
   },
   "outputs": [],
   "source": [
    "TF_IMSIZE = [50,50]\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    directory=dvc_path+\"/split/train\",\n",
    "    target_size=TF_IMSIZE,\n",
    "    batch_size=BATCH,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    directory=dvc_path+\"/split/validation\",\n",
    "    target_size=TF_IMSIZE,\n",
    "    batch_size=BATCH,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:06:17.351961Z",
     "start_time": "2018-03-02T22:06:17.298261Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Keras Sequential model\n",
    "# We do this by passing a list of layers to the Sequential model\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=TF_IMSIZE+[3]),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(20),\n",
    "    keras.layers.Dense(10),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.summary() #summary provides an at-a-glance look at the model we've built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T22:07:00.277113Z",
     "start_time": "2018-03-02T22:06:30.270441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the network\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "steps = 1000 / BATCH\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=.5, patience=1, verbose=1, min_lr=1e-8),\n",
    "    keras.callbacks.EarlyStopping(patience=5, verbose=1),\n",
    "]\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    steps_per_epoch=steps,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=steps,\n",
    "    callbacks=callbacks,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Exercise 3:**\n",
    "\n",
    "Extend the above model as we did before by adding nonlinear activation to the existing layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 4:** \n",
    "\n",
    "Recall that on image tasks, convolutional networks often greatly outperform feedforward networks like those we've created so far. \n",
    "\n",
    "Build a new keras model that uses alternating `Conv2D` layers and `MaxPool2D` (or `AvgPool2D`) layers. Feel free to experiment with adding other layers. Batch normalization often helps deeper networks train and dropout or gaussian noise often helps with overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Unfortunately, for our specific task of distinguishing pictures of cats from pictures of dogs, there aren't many pre-trained models that we can take advantage of. However, many fantastic models have been pre-trained on the popular imagenet dataset which includes hundreds of thousands of images across hundreds of categories. \n",
    "\n",
    "Our last step will be to take a model trained for imagenet and transfer its knowledge to our task. That way, our model doesn't have to learn everything from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 5:** \n",
    "\n",
    "Load mobilenet from `keras.applications.mobilenet`. Initialize it with \n",
    "\n",
    "```\n",
    "mnet_base = MobileNet(\n",
    "    input_shape=(128,128,3),\n",
    "    weights='imagenet',\n",
    "    alpha=.25,\n",
    "    pooling=\"avg\",\n",
    "    include_top=False)\n",
    "```\n",
    "\n",
    "Set all the layers in the model to not be trainable (hint: you can access the layers with `model.layers` and set to not trainable with `layer.trainable = False`). Now, build a new sequential model that uses `mnet_base` as a layer. Run this on our same dataset and compare your results. \n",
    "\n",
    "mobilenet has pre-trained weights for alphas {0.25, .5, .75, 1} and for image sizes {128, 160, 192,192, 224}. Feel free to experiment with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(idp)",
   "language": "python",
   "name": "idp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
